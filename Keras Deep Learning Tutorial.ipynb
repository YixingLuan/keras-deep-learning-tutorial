{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Tutorial\n",
    "\n",
    "#### Abram Hindle\n",
    "#### <abram.hindle@ualberta.ca>\n",
    "#### http://softwareprocess.ca/\n",
    "\n",
    "Slides stolen gracefully from Ben Zittlau\n",
    "\n",
    "Slide content under CC-BY-SA 4.0 and MIT License for source code or the same license as Python3 or Keras. Slide Source code is MIT License as well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### What is machine learning?\n",
    "\n",
    "Building a function from data to classify, predict, group, or represent data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### Machine Learning\n",
    "\n",
    "There are a few kinds of tasks or functions that could help us here.\n",
    "\n",
    "* Classification: given some input, predict the class that it belongs\n",
    "  to. Given a point is it in the red or in the blue?\n",
    "* Regression: Given a point what will its value be? In the case of a\n",
    "  function with a continuous or numerous discrete outputs it might be\n",
    "  appropriate.\n",
    "* Representation: Learn a smaller representation of the input\n",
    "  data. E.g. we have 300 features lets describe them in a 128-bit hash.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro\n",
    "### Motivational Example\n",
    "\n",
    "Imagine we have this data:\n",
    "\n",
    "![2 crescent slices](images/slice.png \"A function we want to learn\n",
    " f(x,y) -> z where z is red\")\n",
    "\n",
    "[See src/genslice.py to see how we made it.](src/genslice.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purpose: make a slight difference of circles be the dataset to learn.\n",
    "import numpy as np\n",
    "gY, gX = np.meshgrid(np.arange(1000)/1000.0,np.arange(1000)/1000.0)\n",
    "\n",
    "def intersect_circle(cx,cy,radius):\n",
    "    equation = (gX - float(cx)) ** 2 + (gY - float(cy)) ** 2\n",
    "    matches = equation < (radius**3) \n",
    "    return matches\n",
    "\n",
    "# rad = 0.1643167672515498\n",
    "rad = 0.3\n",
    "x = intersect_circle(0.5,0.5,rad) ^ intersect_circle(0.51,0.51,rad)\n",
    "\n",
    "def plotit(x):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(x)\n",
    "    plt.savefig('new-slice.png') # was slice.png\n",
    "    plt.imshow(x)\n",
    "    plt.savefig('new-slice.pdf') # was slice.pdf\n",
    "    plt.show()\n",
    "\n",
    "# plotit(x)\n",
    "\n",
    "def mkcol(x):\n",
    "    return x.reshape((x.shape[0]*x.shape[1],1))\n",
    "\n",
    "# make the data set\n",
    "big = np.concatenate((mkcol(gX),mkcol(gY),mkcol(1*x)),axis=1)\n",
    "np.savetxt(\"new-big-slice.csv\", big, delimiter=\",\")\n",
    "\n",
    "# make a 50/50 data set\n",
    "nots = big[big[0:,2]==0.0,]\n",
    "np.random.shuffle(nots)\n",
    "nots = nots[0:1000,]\n",
    "trues = big[big[0:,2]==1.0,]\n",
    "np.random.shuffle(trues)\n",
    "trues = trues[0:1000,]\n",
    "small = np.concatenate((trues,nots))\n",
    "np.savetxt(\"new-small-slice.csv\", small, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### Make your own function\n",
    "\n",
    "``` python\n",
    "def in_circle(x,y,cx,cy,radius):\n",
    "    return (x - float(cx)) ** 2 + (y - float(cy)) ** 2 < radius**2\n",
    "\n",
    "def mysolution(pt,outer=0.3):\n",
    "    return in_circle(pt[0],pt[1],0.5,0.5,outer) and not in_circle(pt[0],pt[1],0.5,0.5,0.1)\n",
    "```\n",
    "\n",
    "```\n",
    ">>> myclasses = np.apply_along_axis(mysolution,1,test[0])\n",
    ">>> print \"My classifier!\"\n",
    "My classifier!\n",
    ">>> print \"%s / %s \" % (sum(myclasses == test[1]),len(test[1]))\n",
    "181 / 200 \n",
    ">>> print theautil.classifications(myclasses,test[1])\n",
    "[('tp', 91), ('tn', 90), ('fp', 19), ('fn', 0)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro \n",
    "### An example classifier\n",
    "\n",
    "1-NN: 1 Nearest Neighbor.\n",
    "\n",
    "Given the data, we produce a function that\n",
    "outputs the CLASS of the nearest neighbour to the input data.\n",
    "\n",
    "Whoever is closer, is the class. 3-NN is 3-nearest neighbors whereby\n",
    "we use voting of the 3 neighbors instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### An example classifier: 1-NN\n",
    "\n",
    "[src/slice-classifier.py](src/slice-classifier.py)\n",
    "\n",
    "``` python\n",
    "def euclid(pt1,pt2):\n",
    "    return sum([ (pt1[i] - pt2[i])**2 for i in range(0,len(pt1)) ])\n",
    "\n",
    "def oneNN(data,labels):\n",
    "    def func(input):\n",
    "        distance = None\n",
    "        label = None\n",
    "        for i in range(0,len(data)):\n",
    "            d = euclid(input,data[i])\n",
    "            if distance == None or d < distance:\n",
    "                distance = d\n",
    "                label = labels[i]\n",
    "        return label\n",
    "    return func\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Intro\n",
    "### An example classifier: 1-NN\n",
    "\n",
    "``` python\n",
    ">>> learner = oneNN(train[0],train[1])\n",
    ">>> \n",
    ">>> oneclasses = np.apply_along_axis(learner,1,test[0])\n",
    ">>> print \"1-NN classifier!\"\n",
    "1-NN classifier!\n",
    ">>> print \"%s / %s \" % (sum(oneclasses == test[1]),len(test[1]))\n",
    "198 / 200 \n",
    ">>> print theautil.classifications(oneclasses,test[1])\n",
    "[('tp', 91), ('tn', 107), ('fp', 2), ('fn', 0)]\n",
    "\n",
    "```\n",
    "\n",
    "1-NN has great performance in this example, but it uses Euclidean\n",
    "distance and the dataset is really quite biased to the positive\n",
    "classes.\n",
    "\n",
    "Thus we showed a simple learner that classifies data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro\n",
    "\n",
    "* That's really interesting performance and it worked but will it\n",
    "  scale and continue to work?\n",
    "\n",
    "* 1-NN doesn't work for all problems. And it is dependent on linear\n",
    "  relationships.\n",
    "\n",
    "* What if our problem is non-linear?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2 Valid: 2\n",
      "Train X: (1620, 2) Y^:(1620, 1)\n",
      "Valid X: (180, 2)  Y^:(180, 1)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# The MIT License (MIT)\n",
    "# \n",
    "# Copyright (c) 2016 Abram Hindle <hindle1@ualberta.ca>, Leif Johnson <leif@lmjohns3.com>\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "# first off we load up some modules we want to use\n",
    "import keras\n",
    "import scipy\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import logging\n",
    "import sys\n",
    "import collections\n",
    "import theautil\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(stream = sys.stderr, level=logging.INFO)\n",
    "\n",
    "mupdates = 1000\n",
    "data = np.loadtxt(\"small-slice.csv\", delimiter=\",\")\n",
    "inputs  = data[0:,0:2].astype(np.float32)\n",
    "outputs = data[0:,2:3].astype(np.int32)\n",
    "\n",
    "theautil.joint_shuffle(inputs,outputs)\n",
    "\n",
    "train_and_valid, test = theautil.split_validation(90, inputs, outputs)\n",
    "train, valid = theautil.split_validation(90, train_and_valid[0], train_and_valid[1])\n",
    "print(\"Train: %s Valid: %s\"%(len(train),len(valid)))\n",
    "print(\"Train X: %s Y^:%s\"%(train[0].shape,train[1].shape))\n",
    "print(\"Valid X: %s  Y^:%s\"%(valid[0].shape,valid[1].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My classifier!\n",
      "175 / 200 \n",
      "[('tp', 102), ('tn', 73), ('fp', 25), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def linit(x):\n",
    "    return x.reshape((len(x),))\n",
    "\n",
    "mltrain = (train[0],linit(train[1]))\n",
    "mlvalid = (valid[0],linit(valid[1]))\n",
    "mltest  = (test[0] ,linit(test[1]))\n",
    "\n",
    "# my solution\n",
    "def in_circle(x,y,cx,cy,radius):\n",
    "    return (x - float(cx)) ** 2 + (y - float(cy)) ** 2 < radius**2\n",
    "\n",
    "def mysolution(pt,outer=0.3):\n",
    "    return in_circle(pt[0],pt[1],0.5,0.5,outer) and not in_circle(pt[0],pt[1],0.5,0.5,0.1)\n",
    "\n",
    "# apply my classifier\n",
    "myclasses = np.apply_along_axis(mysolution,1,mltest[0])\n",
    "print(\"My classifier!\")\n",
    "print(\"%s / %s \" % (sum(myclasses == mltest[1]),len(mltest[1])))\n",
    "print(theautil.classifications(myclasses,mltest[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-NN classifier!\n",
      "200 / 200 \n",
      "[('tp', 102), ('tn', 98), ('fp', 0), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "def euclid(pt1,pt2):\n",
    "    return sum([ (pt1[i] - pt2[i])**2 for i in range(0,len(pt1)) ])\n",
    "\n",
    "def oneNN(data,labels):\n",
    "    def func(input):\n",
    "        distance = None\n",
    "        label = None\n",
    "        for i in range(0,len(data)):\n",
    "            d = euclid(input,data[i])\n",
    "            if distance == None or d < distance:\n",
    "                distance = d\n",
    "                label = labels[i]\n",
    "        return label\n",
    "    return func\n",
    "\n",
    "learner = oneNN(mltrain[0],mltrain[1])\n",
    "\n",
    "oneclasses = np.apply_along_axis(learner,1,mltest[0])\n",
    "print(\"1-NN classifier!\")\n",
    "print(\"%s / %s \" % (sum(oneclasses == mltest[1]),len(mltest[1])))\n",
    "print(theautil.classifications(oneclasses,mltest[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro\n",
    "\n",
    "* Neural networks are popular\n",
    "   * Creating AI for Go\n",
    "   * Labeling Images with cats and dogs\n",
    "   * Speech Recognition\n",
    "   * Text summarization\n",
    "   * [Guitar Transcription](https://peerj.com/preprints/1193.pdf)\n",
    "   * Learn audio from video[1](https://archive.org/details/DeepLearningBitmaptoPCM/)[2](http://softwareprocess.es/blog/blog/2015/08/10/deep-learning-bitmaps-to-pcm/)\n",
    "\n",
    "* Neural networks can not only classify, but they can create content,\n",
    "  they can have complicated outputs.\n",
    "\n",
    "* Neural networks are generative!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "### Machine Learning: Neural Networks\n",
    "\n",
    "Neural networks or \"Artificial Neural Networks\" are a flexible class\n",
    "of non-linear machine learners. They have been found to be quite\n",
    "effective as of late.\n",
    "\n",
    "Neural networks are composed of neurons. These neurons try to emulate\n",
    "biological neurons in the most metaphorical of senses. Given a set of\n",
    "inputs they produce an output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Neurons\n",
    "\n",
    "Neurons have functions.\n",
    "\n",
    "* Rectified Linear Units have been shown to train quite well and\n",
    "  achieve good results. By they aren't easier to differentiate.\n",
    "  f(x) = max(0,x)\n",
    "* Sigmoid functions are slow and were the classical neural network\n",
    "  neuron, but have fallen out of favour. They will work when nothing\n",
    "  else will. f(x) = 1/(1 + e^-x)\n",
    "* Softplus is a RELU that is slower to compute but differentiable.\n",
    "  f(x) = ln(1 + e^x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Neurons\n",
    "\n",
    "![Rectifier and Sigmoid and Softplus](images/Rectifier_and_softplus_functions.svg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Neurons\n",
    "\n",
    "The inputs to a neural network? The outputs of connected nodes times\n",
    "their weight + a bias.\n",
    "\n",
    "neuron(inputs) = neuron_f( sum(weights * inputs) + bias  )\n",
    "\n",
    "![Neuron example](images/neuron.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Multi-layer perceptron\n",
    "\n",
    "Single hidden layer neural network.\n",
    "\n",
    "![Multi-layer perceptron](images/20160208141015.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "There's nothing particularly crazy about deep learning other than it has more hidden layers.\n",
    "\n",
    "These hidden layers allow it to compute state and address the intricacies of complex functions. But each hidden layer adds a lot of search space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deep Learning\n",
    "\n",
    "![Deep network, multiple layers](images/20160208141143.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Search\n",
    "\n",
    "How do we find the different weights?\n",
    "\n",
    "Well we need to search a large space. A 2x3x2 network will have 2*3*2\n",
    "weights + 5 biases (3 hidden, 2 output) resulting in 17\n",
    "parameters. That's already a large search space.\n",
    "\n",
    "Most search algorithms measure their error at a certain point\n",
    "(difference between prediction and actual) and then choose a direction\n",
    "in their search space to travel. They do this by sampling points\n",
    "around themselves in order to compute a gradient or slope and then\n",
    "follow the slope around.\n",
    "\n",
    "Here's a 3D demo of different search algorithms.\n",
    "\n",
    "[Different Search Parameters](http://www.robertsdionne.com/bouncingball/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Let's deep learn on our problem\n",
    "\n",
    "![2 crescent slices](images/slice.png \"A function we want to learn\n",
    " f(x,y) -> z where z is red\")\n",
    "\n",
    "Please open [slice-classifier](./src/slice-classifier.py) and a python\n",
    "interpreter such as bpython. Search for Part 3 around line 100.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Part 3. Let's start using neural networks!\n",
      "########################################################################\n",
      "\n",
      "(1620, 2)\n",
      "(1620, 1)\n",
      "(1620, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('''\n",
    "########################################################################\n",
    "# Part 3. Let's start using neural networks!\n",
    "########################################################################\n",
    "''')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(train[1])\n",
    "train_y = enc.transform(train[1])\n",
    "valid_y = enc.transform(valid[1])\n",
    "test_y = enc.transform(test[1])\n",
    "\n",
    "print(train[0].shape)\n",
    "print(train[1].shape)\n",
    "print(train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1620 samples, validate on 180 samples\n",
      "Epoch 1/100\n",
      "1620/1620 [==============================] - 0s 89us/step - loss: 0.7018 - acc: 0.4994 - val_loss: 0.7809 - val_acc: 0.4889\n",
      "Epoch 2/100\n",
      "1620/1620 [==============================] - 0s 93us/step - loss: 0.6986 - acc: 0.4932 - val_loss: 0.6981 - val_acc: 0.4889\n",
      "Epoch 3/100\n",
      "1620/1620 [==============================] - 0s 92us/step - loss: 0.6941 - acc: 0.4988 - val_loss: 0.6939 - val_acc: 0.5111\n",
      "Epoch 4/100\n",
      "1620/1620 [==============================] - 0s 97us/step - loss: 0.6944 - acc: 0.4988 - val_loss: 0.6927 - val_acc: 0.4889\n",
      "Epoch 5/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.6936 - acc: 0.5111 - val_loss: 0.6967 - val_acc: 0.4389\n",
      "Epoch 6/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.6911 - acc: 0.5185 - val_loss: 0.6991 - val_acc: 0.4889\n",
      "Epoch 7/100\n",
      "1620/1620 [==============================] - 0s 91us/step - loss: 0.6908 - acc: 0.5278 - val_loss: 0.6913 - val_acc: 0.4167\n",
      "Epoch 8/100\n",
      "1620/1620 [==============================] - 0s 96us/step - loss: 0.6910 - acc: 0.5185 - val_loss: 0.6992 - val_acc: 0.4889\n",
      "Epoch 9/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.6901 - acc: 0.5395 - val_loss: 0.6882 - val_acc: 0.4889\n",
      "Epoch 10/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.6881 - acc: 0.5346 - val_loss: 0.6973 - val_acc: 0.4889\n",
      "Epoch 11/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.6869 - acc: 0.5346 - val_loss: 0.6820 - val_acc: 0.6944\n",
      "Epoch 12/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.6857 - acc: 0.5543 - val_loss: 0.6879 - val_acc: 0.4222\n",
      "Epoch 13/100\n",
      "1620/1620 [==============================] - 0s 88us/step - loss: 0.6816 - acc: 0.5451 - val_loss: 0.6798 - val_acc: 0.4667\n",
      "Epoch 14/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.6774 - acc: 0.5580 - val_loss: 0.7218 - val_acc: 0.4889\n",
      "Epoch 15/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.6752 - acc: 0.5660 - val_loss: 0.6713 - val_acc: 0.6944\n",
      "Epoch 16/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.6713 - acc: 0.5815 - val_loss: 0.6682 - val_acc: 0.6111\n",
      "Epoch 17/100\n",
      "1620/1620 [==============================] - 0s 85us/step - loss: 0.6645 - acc: 0.6049 - val_loss: 0.7511 - val_acc: 0.4500\n",
      "Epoch 18/100\n",
      "1620/1620 [==============================] - 0s 85us/step - loss: 0.6594 - acc: 0.5809 - val_loss: 0.6522 - val_acc: 0.6833\n",
      "Epoch 19/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.6497 - acc: 0.6025 - val_loss: 0.6543 - val_acc: 0.4889\n",
      "Epoch 20/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.6420 - acc: 0.6043 - val_loss: 0.6572 - val_acc: 0.6278\n",
      "Epoch 21/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.6326 - acc: 0.6204 - val_loss: 0.6223 - val_acc: 0.5778\n",
      "Epoch 22/100\n",
      "1620/1620 [==============================] - 0s 85us/step - loss: 0.6221 - acc: 0.6278 - val_loss: 0.6110 - val_acc: 0.6167\n",
      "Epoch 23/100\n",
      "1620/1620 [==============================] - 0s 95us/step - loss: 0.6125 - acc: 0.6278 - val_loss: 0.6612 - val_acc: 0.6000\n",
      "Epoch 24/100\n",
      "1620/1620 [==============================] - 0s 88us/step - loss: 0.6021 - acc: 0.6358 - val_loss: 0.6039 - val_acc: 0.7944\n",
      "Epoch 25/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.5894 - acc: 0.6802 - val_loss: 0.6963 - val_acc: 0.5389\n",
      "Epoch 26/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.5694 - acc: 0.7315 - val_loss: 0.5457 - val_acc: 0.7000\n",
      "Epoch 27/100\n",
      "1620/1620 [==============================] - 0s 91us/step - loss: 0.5442 - acc: 0.7605 - val_loss: 0.6643 - val_acc: 0.6444\n",
      "Epoch 28/100\n",
      "1620/1620 [==============================] - 0s 96us/step - loss: 0.5292 - acc: 0.7599 - val_loss: 0.4961 - val_acc: 0.7778\n",
      "Epoch 29/100\n",
      "1620/1620 [==============================] - 0s 96us/step - loss: 0.5043 - acc: 0.7735 - val_loss: 0.5661 - val_acc: 0.7500\n",
      "Epoch 30/100\n",
      "1620/1620 [==============================] - 0s 89us/step - loss: 0.4946 - acc: 0.7691 - val_loss: 0.5578 - val_acc: 0.7000\n",
      "Epoch 31/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.4794 - acc: 0.7833 - val_loss: 0.5027 - val_acc: 0.6833\n",
      "Epoch 32/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.4691 - acc: 0.7827 - val_loss: 0.4456 - val_acc: 0.8111\n",
      "Epoch 33/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.4631 - acc: 0.7833 - val_loss: 0.5090 - val_acc: 0.7556\n",
      "Epoch 34/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.4539 - acc: 0.7969 - val_loss: 0.4119 - val_acc: 0.8500\n",
      "Epoch 35/100\n",
      "1620/1620 [==============================] - 0s 89us/step - loss: 0.4450 - acc: 0.7938 - val_loss: 0.4451 - val_acc: 0.8111\n",
      "Epoch 36/100\n",
      "1620/1620 [==============================] - 0s 91us/step - loss: 0.4416 - acc: 0.7988 - val_loss: 0.4144 - val_acc: 0.8111\n",
      "Epoch 37/100\n",
      "1620/1620 [==============================] - 0s 94us/step - loss: 0.4355 - acc: 0.7957 - val_loss: 0.4110 - val_acc: 0.7889\n",
      "Epoch 38/100\n",
      "1620/1620 [==============================] - 0s 93us/step - loss: 0.4315 - acc: 0.8025 - val_loss: 0.3956 - val_acc: 0.8278\n",
      "Epoch 39/100\n",
      "1620/1620 [==============================] - 0s 98us/step - loss: 0.4261 - acc: 0.8068 - val_loss: 0.3798 - val_acc: 0.8389\n",
      "Epoch 40/100\n",
      "1620/1620 [==============================] - 0s 97us/step - loss: 0.4175 - acc: 0.8074 - val_loss: 0.3730 - val_acc: 0.8444\n",
      "Epoch 41/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.4110 - acc: 0.8173 - val_loss: 0.3768 - val_acc: 0.8556\n",
      "Epoch 42/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.4088 - acc: 0.8241 - val_loss: 0.4041 - val_acc: 0.7944\n",
      "Epoch 43/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.3936 - acc: 0.8321 - val_loss: 0.3399 - val_acc: 0.8722\n",
      "Epoch 44/100\n",
      "1620/1620 [==============================] - 0s 89us/step - loss: 0.3869 - acc: 0.8420 - val_loss: 0.3514 - val_acc: 0.9000\n",
      "Epoch 45/100\n",
      "1620/1620 [==============================] - 0s 103us/step - loss: 0.3801 - acc: 0.8463 - val_loss: 0.3442 - val_acc: 0.8722\n",
      "Epoch 46/100\n",
      "1620/1620 [==============================] - 0s 89us/step - loss: 0.3685 - acc: 0.8599 - val_loss: 0.4145 - val_acc: 0.8056\n",
      "Epoch 47/100\n",
      "1620/1620 [==============================] - 0s 85us/step - loss: 0.3596 - acc: 0.8623 - val_loss: 0.3279 - val_acc: 0.9056\n",
      "Epoch 48/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.3585 - acc: 0.8617 - val_loss: 0.3818 - val_acc: 0.8000\n",
      "Epoch 49/100\n",
      "1620/1620 [==============================] - 0s 91us/step - loss: 0.3498 - acc: 0.8617 - val_loss: 0.2830 - val_acc: 0.9056\n",
      "Epoch 50/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.3398 - acc: 0.8673 - val_loss: 0.3922 - val_acc: 0.8167\n",
      "Epoch 51/100\n",
      "1620/1620 [==============================] - 0s 95us/step - loss: 0.3381 - acc: 0.8685 - val_loss: 0.2830 - val_acc: 0.9167\n",
      "Epoch 52/100\n",
      "1620/1620 [==============================] - 0s 92us/step - loss: 0.3272 - acc: 0.8722 - val_loss: 0.2766 - val_acc: 0.9000\n",
      "Epoch 53/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.3188 - acc: 0.8821 - val_loss: 0.4733 - val_acc: 0.6944\n",
      "Epoch 54/100\n",
      "1620/1620 [==============================] - 0s 88us/step - loss: 0.3201 - acc: 0.8735 - val_loss: 0.2994 - val_acc: 0.8611\n",
      "Epoch 55/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.3118 - acc: 0.8840 - val_loss: 0.2677 - val_acc: 0.9111\n",
      "Epoch 56/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.3104 - acc: 0.8802 - val_loss: 0.5247 - val_acc: 0.6944\n",
      "Epoch 57/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.3122 - acc: 0.8772 - val_loss: 0.2507 - val_acc: 0.9056\n",
      "Epoch 58/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.3104 - acc: 0.8833 - val_loss: 0.2498 - val_acc: 0.9111\n",
      "Epoch 59/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.2996 - acc: 0.8858 - val_loss: 0.2632 - val_acc: 0.9056\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.3043 - acc: 0.8938 - val_loss: 0.3079 - val_acc: 0.8556\n",
      "Epoch 61/100\n",
      "1620/1620 [==============================] - 0s 94us/step - loss: 0.2983 - acc: 0.8920 - val_loss: 0.2517 - val_acc: 0.9111\n",
      "Epoch 62/100\n",
      "1620/1620 [==============================] - 0s 89us/step - loss: 0.2940 - acc: 0.8932 - val_loss: 0.4370 - val_acc: 0.7444\n",
      "Epoch 63/100\n",
      "1620/1620 [==============================] - 0s 92us/step - loss: 0.2911 - acc: 0.8914 - val_loss: 0.2736 - val_acc: 0.8889\n",
      "Epoch 64/100\n",
      "1620/1620 [==============================] - 0s 93us/step - loss: 0.2961 - acc: 0.8901 - val_loss: 0.4881 - val_acc: 0.6944\n",
      "Epoch 65/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.2959 - acc: 0.8858 - val_loss: 0.2989 - val_acc: 0.8833\n",
      "Epoch 66/100\n",
      "1620/1620 [==============================] - 0s 96us/step - loss: 0.2887 - acc: 0.8988 - val_loss: 0.2380 - val_acc: 0.9111\n",
      "Epoch 67/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.2861 - acc: 0.8944 - val_loss: 0.2692 - val_acc: 0.9000\n",
      "Epoch 68/100\n",
      "1620/1620 [==============================] - 0s 85us/step - loss: 0.2849 - acc: 0.8914 - val_loss: 0.2542 - val_acc: 0.9167\n",
      "Epoch 69/100\n",
      "1620/1620 [==============================] - 0s 95us/step - loss: 0.2859 - acc: 0.8920 - val_loss: 0.2465 - val_acc: 0.9111\n",
      "Epoch 70/100\n",
      "1620/1620 [==============================] - 0s 93us/step - loss: 0.2876 - acc: 0.8914 - val_loss: 0.4317 - val_acc: 0.7556\n",
      "Epoch 71/100\n",
      "1620/1620 [==============================] - 0s 97us/step - loss: 0.2903 - acc: 0.8901 - val_loss: 0.2384 - val_acc: 0.9111\n",
      "Epoch 72/100\n",
      "1620/1620 [==============================] - 0s 92us/step - loss: 0.2872 - acc: 0.8938 - val_loss: 0.2747 - val_acc: 0.8833\n",
      "Epoch 73/100\n",
      "1620/1620 [==============================] - 0s 89us/step - loss: 0.2842 - acc: 0.8920 - val_loss: 0.2619 - val_acc: 0.8833\n",
      "Epoch 74/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.2787 - acc: 0.8920 - val_loss: 0.7325 - val_acc: 0.7056\n",
      "Epoch 75/100\n",
      "1620/1620 [==============================] - 0s 88us/step - loss: 0.2842 - acc: 0.8957 - val_loss: 0.2438 - val_acc: 0.9167\n",
      "Epoch 76/100\n",
      "1620/1620 [==============================] - 0s 96us/step - loss: 0.2752 - acc: 0.8988 - val_loss: 0.2305 - val_acc: 0.9167\n",
      "Epoch 77/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.2767 - acc: 0.8951 - val_loss: 0.2272 - val_acc: 0.9111\n",
      "Epoch 78/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.2740 - acc: 0.8975 - val_loss: 0.2300 - val_acc: 0.9167\n",
      "Epoch 79/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.2724 - acc: 0.8994 - val_loss: 0.2535 - val_acc: 0.9167\n",
      "Epoch 80/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.2726 - acc: 0.9019 - val_loss: 0.2234 - val_acc: 0.9167\n",
      "Epoch 81/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.2705 - acc: 0.8981 - val_loss: 0.2350 - val_acc: 0.9167\n",
      "Epoch 82/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.2657 - acc: 0.9062 - val_loss: 0.2474 - val_acc: 0.8944\n",
      "Epoch 83/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.2675 - acc: 0.9049 - val_loss: 0.2224 - val_acc: 0.9167\n",
      "Epoch 84/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.2645 - acc: 0.9074 - val_loss: 0.2304 - val_acc: 0.9167\n",
      "Epoch 85/100\n",
      "1620/1620 [==============================] - 0s 87us/step - loss: 0.2656 - acc: 0.9006 - val_loss: 0.2399 - val_acc: 0.8944\n",
      "Epoch 86/100\n",
      "1620/1620 [==============================] - 0s 98us/step - loss: 0.2622 - acc: 0.9006 - val_loss: 0.2792 - val_acc: 0.8722\n",
      "Epoch 87/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.2639 - acc: 0.9093 - val_loss: 0.2454 - val_acc: 0.8944\n",
      "Epoch 88/100\n",
      "1620/1620 [==============================] - 0s 88us/step - loss: 0.2687 - acc: 0.9025 - val_loss: 0.2272 - val_acc: 0.9222\n",
      "Epoch 89/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.2687 - acc: 0.8975 - val_loss: 0.4149 - val_acc: 0.8333\n",
      "Epoch 90/100\n",
      "1620/1620 [==============================] - 0s 107us/step - loss: 0.2750 - acc: 0.8969 - val_loss: 0.2578 - val_acc: 0.9056\n",
      "Epoch 91/100\n",
      "1620/1620 [==============================] - 0s 88us/step - loss: 0.2637 - acc: 0.9049 - val_loss: 0.2892 - val_acc: 0.8556\n",
      "Epoch 92/100\n",
      "1620/1620 [==============================] - 0s 88us/step - loss: 0.2580 - acc: 0.9068 - val_loss: 0.2422 - val_acc: 0.9000\n",
      "Epoch 93/100\n",
      "1620/1620 [==============================] - 0s 95us/step - loss: 0.2631 - acc: 0.9043 - val_loss: 0.2358 - val_acc: 0.9000\n",
      "Epoch 94/100\n",
      "1620/1620 [==============================] - 0s 86us/step - loss: 0.2555 - acc: 0.9136 - val_loss: 0.2197 - val_acc: 0.9222\n",
      "Epoch 95/100\n",
      "1620/1620 [==============================] - 0s 85us/step - loss: 0.2590 - acc: 0.9031 - val_loss: 0.2165 - val_acc: 0.9111\n",
      "Epoch 96/100\n",
      "1620/1620 [==============================] - 0s 88us/step - loss: 0.2557 - acc: 0.9093 - val_loss: 0.2214 - val_acc: 0.9222\n",
      "Epoch 97/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.2542 - acc: 0.9080 - val_loss: 0.2124 - val_acc: 0.9222\n",
      "Epoch 98/100\n",
      "1620/1620 [==============================] - 0s 85us/step - loss: 0.2546 - acc: 0.9093 - val_loss: 0.2222 - val_acc: 0.9222\n",
      "Epoch 99/100\n",
      "1620/1620 [==============================] - 0s 90us/step - loss: 0.2559 - acc: 0.9049 - val_loss: 0.2266 - val_acc: 0.9500\n",
      "Epoch 100/100\n",
      "1620/1620 [==============================] - 0s 85us/step - loss: 0.2494 - acc: 0.9154 - val_loss: 0.2151 - val_acc: 0.9278\n"
     ]
    }
   ],
   "source": [
    "# rerunning this will produce different results\n",
    "# try different combos here\n",
    "net = Sequential()\n",
    "net.add(Dense(16,input_shape=(2,),activation=\"sigmoid\"))\n",
    "net.add(Dense(16,activation=\"relu\"))\n",
    "net.add(Dense(2,activation=\"softmax\"))\n",
    "opt = SGD(lr=0.1)\n",
    "# opt = Adam(lr=0.1)\n",
    "net.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "history = net.fit(train[0], train_y, validation_data=(valid[0], valid_y),\n",
    "\t            epochs=100, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner on the test set\n",
      "200/200 [==============================] - 0s 54us/step\n",
      "Scores: [0.2766148710250855, 0.895]\n",
      "(200, 2)\n",
      "[[9.4758965e-02 9.0524101e-01]\n",
      " [1.2368786e-01 8.7631214e-01]\n",
      " [9.9979770e-01 2.0228619e-04]\n",
      " [1.0000000e+00 1.9776802e-08]\n",
      " [1.6064729e-01 8.3935273e-01]\n",
      " [9.9999821e-01 1.8081136e-06]\n",
      " [9.9463791e-01 5.3620674e-03]\n",
      " [9.9937552e-01 6.2449591e-04]\n",
      " [1.1010743e-01 8.8989258e-01]\n",
      " [9.9959230e-01 4.0771632e-04]]\n",
      "179 / 200 \n",
      "Counter({1: 123, 0: 77})\n",
      "[('tp', 102), ('tn', 77), ('fp', 21), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Learner on the test set\")\n",
    "score = net.evaluate(test[0], test_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "predictit = net.predict(test[0])\n",
    "print(predictit.shape)\n",
    "print(predictit[0:10,])\n",
    "classify = net.predict_classes(test[0])\n",
    "\n",
    "print(\"%s / %s \" % (np.sum(classify == mltest[1]),len(mltest[1])))\n",
    "print(collections.Counter(classify))\n",
    "print(theautil.classifications(classify,mltest[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now on more unseen data that isn't 50/50\n",
      "2498 / 3000 \n",
      "Counter({0: 2471, 1: 529})\n",
      "[('tp', 27), ('tn', 2471), ('fp', 502), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def real_function(pt):\n",
    "    rad = 0.1643167672515498\n",
    "    in1 = in_circle(pt[0],pt[1],0.5,0.5,rad)\n",
    "    in2 = in_circle(pt[0],pt[1],0.51,0.51,rad)\n",
    "    return in1 ^ in2\n",
    "\n",
    "print(\"And now on more unseen data that isn't 50/50\")\n",
    "\n",
    "bigtest = np.random.uniform(size=(3000,2)).astype(np.float32)\n",
    "biglab = np.apply_along_axis(real_function,1,bigtest).astype(np.int32)\n",
    "\n",
    "classify = net.predict_classes(bigtest)\n",
    "print(\"%s / %s \" % (sum(classify == biglab),len(biglab)))\n",
    "print(collections.Counter(classify))\n",
    "print(theautil.classifications(classify,biglab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Now let's discuss posing problems for neural networks\n",
    "\n",
    "* Scaling inputs: Scaling can sometimes help, so can\n",
    "  standardization. This means constraining values or re-centering\n",
    "  them. It depends on your problem and it is worth trying.\n",
    "\n",
    "* E.g. min max scaling:\n",
    "\n",
    "``` python\n",
    "def min_max_scale(data):\n",
    "    '''scales data by minimum and maximum values between 0 and 1'''\n",
    "    dmin = np.min(data)\n",
    "    return (data - dmin)/(np.max(data) - dmin)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "* [posing.py](src/posing.py) tries to show the problem of taking\n",
    "  random input data and determine what distribution it comes from.\n",
    "  That is what function can produce these random values.\n",
    "\n",
    "* Let's open up [posing.py](src/posing.py) and get an interpreter\n",
    "  going.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of how to pose the problem and how different formulations\n",
    "# lead to different results!\n",
    "#\n",
    "# The MIT License (MIT)\n",
    "# \n",
    "# Copyright (c) 2016 Abram Hindle <hindle1@ualberta.ca>, Leif Johnson <leif@lmjohns3.com>\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "# first off we load up some modules we want to use\n",
    "import keras\n",
    "import scipy\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import logging\n",
    "import sys\n",
    "from numpy.random import power, normal, lognormal, uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import theanets\n",
    "\n",
    "# What are we going to do?\n",
    "# - we're going to generate data derived from 4 different distributions\n",
    "# - we're going to scale that data\n",
    "# - we're going to create a RBM (1 hidden layer neural network)\n",
    "# - we're going to train it to classify data as belonging to one of these distributions\n",
    "\n",
    "# maximum number of iterations before we bail\n",
    "mupdates = 1000\n",
    "\n",
    "# setup logging\n",
    "logging.basicConfig(stream = sys.stderr, level=logging.INFO)\n",
    "\n",
    "# how we pose our problem to the deep belief network matters.\n",
    "\n",
    "# lets make the task easier by scaling all values between 0 and 1\n",
    "def min_max_scale(data):\n",
    "    '''scales data by minimum and maximum values between 0 and 1'''\n",
    "    dmin = np.min(data)\n",
    "    return (data - dmin)/(np.max(data) - dmin)\n",
    "\n",
    "# how many samples per each distribution\n",
    "bsize    = 100 \n",
    "\n",
    "# poor man's enum\n",
    "LOGNORMAL=0\n",
    "POWER=1\n",
    "NORM=2\n",
    "UNIFORM=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experiment 1\n",
    "\n",
    "* Given 1 single sample what distribution does it come from?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Experiment 1: can we classify single samples?\n",
      "#\n",
      "#\n",
      "#########################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "########################################################################\n",
    "# Experiment 1: can we classify single samples?\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "''')\n",
    "\n",
    "def make_dataset1():\n",
    "    '''Make a dataset of single samples with labels from which distribution they come from'''\n",
    "    # now lets make some samples \n",
    "    lns      = min_max_scale(lognormal(size=bsize)) #log normal\n",
    "    powers   = min_max_scale(power(0.1,size=bsize)) #power law\n",
    "    norms    = min_max_scale(normal(size=bsize))    #normal\n",
    "    uniforms = min_max_scale(uniform(size=bsize))    #uniform\n",
    "    # add our data together\n",
    "    data = np.concatenate((lns,powers,norms,uniforms))\n",
    "    \n",
    "    # concatenate our labels\n",
    "    labels = np.concatenate((\n",
    "        (np.repeat(LOGNORMAL,bsize)),\n",
    "        (np.repeat(POWER,bsize)),\n",
    "        (np.repeat(NORM,bsize)),\n",
    "        (np.repeat(UNIFORM,bsize))))\n",
    "    tsize = len(labels)\n",
    "    \n",
    "    # make sure dimensionality and types are right\n",
    "    data = data.reshape((len(data),1))\n",
    "    data = data.astype(np.float32)\n",
    "    labels = labels.astype(np.int32)\n",
    "    labels = labels.reshape((len(data),))\n",
    "    \n",
    "    return data, labels, tsize\n",
    "\n",
    "# this will be the training data and validation data\n",
    "data, labels, tsize = make_dataset1()\n",
    "\n",
    "\n",
    "# this is the test data, this is kept separate to prove we can\n",
    "# actually work on the data we claim we can.\n",
    "#\n",
    "# Without test data, you might just have great performance on the\n",
    "# train set.\n",
    "test_data, test_labels, _ = make_dataset1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 1)\n",
      "(360,)\n",
      "(360, 4)\n",
      "(40, 4)\n",
      "(400, 4)\n"
     ]
    }
   ],
   "source": [
    "# utilities\n",
    "\n",
    "# now lets shuffle\n",
    "# If we're going to select a validation set we probably want to shuffle\n",
    "def joint_shuffle(arr1,arr2):\n",
    "    assert len(arr1) == len(arr2)\n",
    "    indices = np.arange(len(arr1))\n",
    "    np.random.shuffle(indices)\n",
    "    arr1[0:len(arr1)] = arr1[indices]\n",
    "    arr2[0:len(arr2)] = arr2[indices]\n",
    "\n",
    "# our data and labels are shuffled together\n",
    "joint_shuffle(data,labels)\n",
    "\n",
    "def split_validation(percent, data, labels):\n",
    "    ''' \n",
    "    split_validation splits a dataset of data and labels into\n",
    "    2 partitions at the percent mark\n",
    "    percent should be an int between 1 and 99\n",
    "    '''\n",
    "    s = int(percent * len(data) / 100)\n",
    "    tdata = data[0:s]\n",
    "    vdata = data[s:]\n",
    "    tlabels = labels[0:s]\n",
    "    vlabels = labels[s:]\n",
    "    return ((tdata,tlabels),(vdata,vlabels))\n",
    "\n",
    "# make a validation set from the train set\n",
    "train1, valid1 = split_validation(90, data, labels)\n",
    "\n",
    "print(train1[0].shape)\n",
    "print(train1[1].shape)\n",
    "\n",
    "enc1 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc1.fit(train1[1].reshape(len(train1[1]),1))\n",
    "train1_y = enc1.transform(train1[1].reshape(len(train1[1]),1))\n",
    "print(train1_y.shape)\n",
    "valid1_y = enc1.transform(valid1[1].reshape(len(valid1[1]),1))\n",
    "print(valid1_y.shape)\n",
    "test1_y = enc1.transform(test_labels.reshape(len(test_labels),1))\n",
    "print(test1_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're building a MLP of 1 input layer node, 4 hidden layer nodes, and an output layer of 4 nodes. The output layer has 4 nodes because we have 4 classes that the neural network will output.\n",
      "Train on 360 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "360/360 [==============================] - 0s 91us/step - loss: 1.4324 - acc: 0.1361 - val_loss: 1.4051 - val_acc: 0.0500\n",
      "Epoch 2/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 1.4057 - acc: 0.1278 - val_loss: 1.4079 - val_acc: 0.2000\n",
      "Epoch 3/100\n",
      "360/360 [==============================] - 0s 94us/step - loss: 1.4017 - acc: 0.2167 - val_loss: 1.4019 - val_acc: 0.0500\n",
      "Epoch 4/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.4007 - acc: 0.1639 - val_loss: 1.4022 - val_acc: 0.1500\n",
      "Epoch 5/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3994 - acc: 0.2056 - val_loss: 1.3970 - val_acc: 0.0250\n",
      "Epoch 6/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3985 - acc: 0.1500 - val_loss: 1.4055 - val_acc: 0.2000\n",
      "Epoch 7/100\n",
      "360/360 [==============================] - 0s 96us/step - loss: 1.3977 - acc: 0.1972 - val_loss: 1.4013 - val_acc: 0.2000\n",
      "Epoch 8/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.3966 - acc: 0.1556 - val_loss: 1.3973 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "360/360 [==============================] - 0s 120us/step - loss: 1.3947 - acc: 0.2167 - val_loss: 1.3931 - val_acc: 0.2750\n",
      "Epoch 10/100\n",
      "360/360 [==============================] - 0s 121us/step - loss: 1.3954 - acc: 0.1750 - val_loss: 1.3898 - val_acc: 0.2750\n",
      "Epoch 11/100\n",
      "360/360 [==============================] - 0s 121us/step - loss: 1.3931 - acc: 0.1861 - val_loss: 1.3926 - val_acc: 0.2000\n",
      "Epoch 12/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3936 - acc: 0.2333 - val_loss: 1.3948 - val_acc: 0.1500\n",
      "Epoch 13/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 1.3923 - acc: 0.2389 - val_loss: 1.3957 - val_acc: 0.2000\n",
      "Epoch 14/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3920 - acc: 0.2194 - val_loss: 1.3974 - val_acc: 0.2000\n",
      "Epoch 15/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.3914 - acc: 0.2028 - val_loss: 1.3923 - val_acc: 0.2000\n",
      "Epoch 16/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 1.3902 - acc: 0.1917 - val_loss: 1.3855 - val_acc: 0.3000\n",
      "Epoch 17/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 1.3909 - acc: 0.2306 - val_loss: 1.3866 - val_acc: 0.2000\n",
      "Epoch 18/100\n",
      "360/360 [==============================] - 0s 110us/step - loss: 1.3896 - acc: 0.2306 - val_loss: 1.3931 - val_acc: 0.2000\n",
      "Epoch 19/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.3879 - acc: 0.2389 - val_loss: 1.3809 - val_acc: 0.4000\n",
      "Epoch 20/100\n",
      "360/360 [==============================] - 0s 88us/step - loss: 1.3890 - acc: 0.2278 - val_loss: 1.3797 - val_acc: 0.2750\n",
      "Epoch 21/100\n",
      "360/360 [==============================] - 0s 91us/step - loss: 1.3869 - acc: 0.2722 - val_loss: 1.3890 - val_acc: 0.2000\n",
      "Epoch 22/100\n",
      "360/360 [==============================] - 0s 93us/step - loss: 1.3862 - acc: 0.2583 - val_loss: 1.3805 - val_acc: 0.2750\n",
      "Epoch 23/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.3867 - acc: 0.2611 - val_loss: 1.3887 - val_acc: 0.2000\n",
      "Epoch 24/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.3869 - acc: 0.2611 - val_loss: 1.3817 - val_acc: 0.4000\n",
      "Epoch 25/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3859 - acc: 0.2944 - val_loss: 1.3841 - val_acc: 0.2000\n",
      "Epoch 26/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3834 - acc: 0.2583 - val_loss: 1.3902 - val_acc: 0.3500\n",
      "Epoch 27/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3826 - acc: 0.2833 - val_loss: 1.3815 - val_acc: 0.4500\n",
      "Epoch 28/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3826 - acc: 0.3083 - val_loss: 1.3795 - val_acc: 0.3750\n",
      "Epoch 29/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.3812 - acc: 0.3250 - val_loss: 1.3828 - val_acc: 0.3000\n",
      "Epoch 30/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3815 - acc: 0.2889 - val_loss: 1.3818 - val_acc: 0.3000\n",
      "Epoch 31/100\n",
      "360/360 [==============================] - 0s 96us/step - loss: 1.3791 - acc: 0.3361 - val_loss: 1.3776 - val_acc: 0.3750\n",
      "Epoch 32/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.3780 - acc: 0.3278 - val_loss: 1.3711 - val_acc: 0.3500\n",
      "Epoch 33/100\n",
      "360/360 [==============================] - 0s 91us/step - loss: 1.3783 - acc: 0.2889 - val_loss: 1.3732 - val_acc: 0.4750\n",
      "Epoch 34/100\n",
      "360/360 [==============================] - 0s 93us/step - loss: 1.3751 - acc: 0.3750 - val_loss: 1.3716 - val_acc: 0.3000\n",
      "Epoch 35/100\n",
      "360/360 [==============================] - 0s 95us/step - loss: 1.3749 - acc: 0.3278 - val_loss: 1.3670 - val_acc: 0.3500\n",
      "Epoch 36/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 1.3741 - acc: 0.2889 - val_loss: 1.3694 - val_acc: 0.3500\n",
      "Epoch 37/100\n",
      "360/360 [==============================] - 0s 96us/step - loss: 1.3714 - acc: 0.3194 - val_loss: 1.3753 - val_acc: 0.2750\n",
      "Epoch 38/100\n",
      "360/360 [==============================] - 0s 93us/step - loss: 1.3712 - acc: 0.3194 - val_loss: 1.3624 - val_acc: 0.3750\n",
      "Epoch 39/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.3679 - acc: 0.4000 - val_loss: 1.3564 - val_acc: 0.4250\n",
      "Epoch 40/100\n",
      "360/360 [==============================] - 0s 93us/step - loss: 1.3675 - acc: 0.3611 - val_loss: 1.3566 - val_acc: 0.4750\n",
      "Epoch 41/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3652 - acc: 0.4056 - val_loss: 1.3538 - val_acc: 0.3750\n",
      "Epoch 42/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3630 - acc: 0.3361 - val_loss: 1.3492 - val_acc: 0.3750\n",
      "Epoch 43/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3613 - acc: 0.3778 - val_loss: 1.3447 - val_acc: 0.3500\n",
      "Epoch 44/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.3591 - acc: 0.3611 - val_loss: 1.3426 - val_acc: 0.4500\n",
      "Epoch 45/100\n",
      "360/360 [==============================] - 0s 95us/step - loss: 1.3562 - acc: 0.3917 - val_loss: 1.3352 - val_acc: 0.5000\n",
      "Epoch 46/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.3529 - acc: 0.3722 - val_loss: 1.3348 - val_acc: 0.4500\n",
      "Epoch 47/100\n",
      "360/360 [==============================] - 0s 88us/step - loss: 1.3504 - acc: 0.4056 - val_loss: 1.3273 - val_acc: 0.4500\n",
      "Epoch 48/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3489 - acc: 0.3972 - val_loss: 1.3253 - val_acc: 0.4750\n",
      "Epoch 49/100\n",
      "360/360 [==============================] - 0s 88us/step - loss: 1.3445 - acc: 0.3722 - val_loss: 1.3245 - val_acc: 0.3500\n",
      "Epoch 50/100\n",
      "360/360 [==============================] - 0s 88us/step - loss: 1.3407 - acc: 0.3889 - val_loss: 1.3205 - val_acc: 0.3750\n",
      "Epoch 51/100\n",
      "360/360 [==============================] - 0s 91us/step - loss: 1.3366 - acc: 0.4194 - val_loss: 1.3080 - val_acc: 0.4750\n",
      "Epoch 52/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3347 - acc: 0.4250 - val_loss: 1.3046 - val_acc: 0.5500\n",
      "Epoch 53/100\n",
      "360/360 [==============================] - 0s 91us/step - loss: 1.3313 - acc: 0.4000 - val_loss: 1.2991 - val_acc: 0.4500\n",
      "Epoch 54/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3286 - acc: 0.3944 - val_loss: 1.3009 - val_acc: 0.3750\n",
      "Epoch 55/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.3235 - acc: 0.3806 - val_loss: 1.2884 - val_acc: 0.3750\n",
      "Epoch 56/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3192 - acc: 0.4000 - val_loss: 1.2836 - val_acc: 0.5500\n",
      "Epoch 57/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.3137 - acc: 0.4250 - val_loss: 1.2722 - val_acc: 0.5500\n",
      "Epoch 58/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.3112 - acc: 0.3917 - val_loss: 1.2677 - val_acc: 0.4500\n",
      "Epoch 59/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.3070 - acc: 0.3889 - val_loss: 1.2675 - val_acc: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.3009 - acc: 0.4056 - val_loss: 1.2606 - val_acc: 0.3750\n",
      "Epoch 61/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.2971 - acc: 0.4056 - val_loss: 1.2494 - val_acc: 0.4500\n",
      "Epoch 62/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.2935 - acc: 0.4139 - val_loss: 1.2409 - val_acc: 0.4500\n",
      "Epoch 63/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.2879 - acc: 0.4111 - val_loss: 1.2270 - val_acc: 0.4750\n",
      "Epoch 64/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.2844 - acc: 0.4306 - val_loss: 1.2239 - val_acc: 0.4500\n",
      "Epoch 65/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.2792 - acc: 0.4000 - val_loss: 1.2171 - val_acc: 0.4750\n",
      "Epoch 66/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.2740 - acc: 0.4111 - val_loss: 1.2076 - val_acc: 0.4500\n",
      "Epoch 67/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.2704 - acc: 0.4000 - val_loss: 1.2000 - val_acc: 0.5250\n",
      "Epoch 68/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.2653 - acc: 0.4250 - val_loss: 1.1998 - val_acc: 0.5500\n",
      "Epoch 69/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.2621 - acc: 0.3944 - val_loss: 1.1927 - val_acc: 0.3750\n",
      "Epoch 70/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.2570 - acc: 0.4389 - val_loss: 1.1830 - val_acc: 0.3750\n",
      "Epoch 71/100\n",
      "360/360 [==============================] - 0s 91us/step - loss: 1.2529 - acc: 0.4000 - val_loss: 1.1799 - val_acc: 0.3750\n",
      "Epoch 72/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.2493 - acc: 0.4361 - val_loss: 1.1618 - val_acc: 0.6250\n",
      "Epoch 73/100\n",
      "360/360 [==============================] - 0s 95us/step - loss: 1.2424 - acc: 0.4306 - val_loss: 1.1580 - val_acc: 0.4500\n",
      "Epoch 74/100\n",
      "360/360 [==============================] - 0s 94us/step - loss: 1.2396 - acc: 0.4306 - val_loss: 1.1485 - val_acc: 0.4750\n",
      "Epoch 75/100\n",
      "360/360 [==============================] - 0s 93us/step - loss: 1.2354 - acc: 0.4167 - val_loss: 1.1428 - val_acc: 0.5000\n",
      "Epoch 76/100\n",
      "360/360 [==============================] - 0s 94us/step - loss: 1.2311 - acc: 0.4083 - val_loss: 1.1368 - val_acc: 0.5750\n",
      "Epoch 77/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.2293 - acc: 0.4278 - val_loss: 1.1362 - val_acc: 0.5250\n",
      "Epoch 78/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.2253 - acc: 0.4306 - val_loss: 1.1207 - val_acc: 0.6250\n",
      "Epoch 79/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.2213 - acc: 0.4444 - val_loss: 1.1182 - val_acc: 0.5000\n",
      "Epoch 80/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.2176 - acc: 0.4361 - val_loss: 1.1108 - val_acc: 0.5500\n",
      "Epoch 81/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 1.2157 - acc: 0.4278 - val_loss: 1.1150 - val_acc: 0.3750\n",
      "Epoch 82/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 1.2123 - acc: 0.4500 - val_loss: 1.0983 - val_acc: 0.5000\n",
      "Epoch 83/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.2074 - acc: 0.4167 - val_loss: 1.0888 - val_acc: 0.5500\n",
      "Epoch 84/100\n",
      "360/360 [==============================] - 0s 93us/step - loss: 1.2057 - acc: 0.4528 - val_loss: 1.0842 - val_acc: 0.6000\n",
      "Epoch 85/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.2029 - acc: 0.4361 - val_loss: 1.0747 - val_acc: 0.5750\n",
      "Epoch 86/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.2009 - acc: 0.4722 - val_loss: 1.0800 - val_acc: 0.5000\n",
      "Epoch 87/100\n",
      "360/360 [==============================] - 0s 93us/step - loss: 1.1986 - acc: 0.4139 - val_loss: 1.0748 - val_acc: 0.5750\n",
      "Epoch 88/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.1976 - acc: 0.4472 - val_loss: 1.0580 - val_acc: 0.5500\n",
      "Epoch 89/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.1944 - acc: 0.4556 - val_loss: 1.0541 - val_acc: 0.5250\n",
      "Epoch 90/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.1918 - acc: 0.4306 - val_loss: 1.0546 - val_acc: 0.5000\n",
      "Epoch 91/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.1896 - acc: 0.4333 - val_loss: 1.0478 - val_acc: 0.4750\n",
      "Epoch 92/100\n",
      "360/360 [==============================] - 0s 89us/step - loss: 1.1887 - acc: 0.4139 - val_loss: 1.0438 - val_acc: 0.5250\n",
      "Epoch 93/100\n",
      "360/360 [==============================] - 0s 131us/step - loss: 1.1865 - acc: 0.4000 - val_loss: 1.0395 - val_acc: 0.5500\n",
      "Epoch 94/100\n",
      "360/360 [==============================] - 0s 134us/step - loss: 1.1858 - acc: 0.4611 - val_loss: 1.0400 - val_acc: 0.5500\n",
      "Epoch 95/100\n",
      "360/360 [==============================] - 0s 122us/step - loss: 1.1845 - acc: 0.4861 - val_loss: 1.0378 - val_acc: 0.5250\n",
      "Epoch 96/100\n",
      "360/360 [==============================] - 0s 91us/step - loss: 1.1835 - acc: 0.4361 - val_loss: 1.0333 - val_acc: 0.5500\n",
      "Epoch 97/100\n",
      "360/360 [==============================] - 0s 90us/step - loss: 1.1801 - acc: 0.4500 - val_loss: 1.0256 - val_acc: 0.5750\n",
      "Epoch 98/100\n",
      "360/360 [==============================] - 0s 91us/step - loss: 1.1802 - acc: 0.4583 - val_loss: 1.0226 - val_acc: 0.5750\n",
      "Epoch 99/100\n",
      "360/360 [==============================] - 0s 96us/step - loss: 1.1789 - acc: 0.4944 - val_loss: 1.0248 - val_acc: 0.5750\n",
      "Epoch 100/100\n",
      "360/360 [==============================] - 0s 92us/step - loss: 1.1760 - acc: 0.4278 - val_loss: 1.0153 - val_acc: 0.6250\n",
      "[('tp', 65), ('tn', 57), ('fp', 28), ('fn', 17)]\n",
      "400/400 [==============================] - 0s 36us/step\n",
      "Scores: [1.1935731315612792, 0.51]\n"
     ]
    }
   ],
   "source": [
    "# build our classifier\n",
    "\n",
    "print(\"We're building a MLP of 1 input layer node, 4 hidden layer nodes, and an output layer of 4 nodes. The output layer has 4 nodes because we have 4 classes that the neural network will output.\")\n",
    "cnet = Sequential()\n",
    "cnet.add(Dense(4,input_shape=(1,),activation=\"sigmoid\"))\n",
    "cnet.add(Dense(4,activation=\"softmax\"))\n",
    "copt = SGD(lr=0.1)\n",
    "# opt = Adam(lr=0.1)\n",
    "cnet.compile(loss=\"categorical_crossentropy\", optimizer=copt, metrics=[\"accuracy\"])\n",
    "history = cnet.fit(train1[0], train1_y, validation_data=(valid1[0], valid1_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "\n",
    "#score = cnet.evaluate(test_data, test_labels)\n",
    "#print(\"Scores: %s\" % score)\n",
    "classify = cnet.predict_classes(test_data)\n",
    "print(theautil.classifications(classify,test_labels))\n",
    "score = cnet.evaluate(test_data, test1_y)\n",
    "print(\"Scores: %s\" % score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experiment 2\n",
    "\n",
    "* Given 40 samples what distribution does it come from?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Experiment 2: can we classify a sample of data?\n",
      "#\n",
      "#\n",
      "#########################################################################\n",
      "\n",
      "In this example we're going to input 40 values from a single distribution, and we'll see if we can classify the distribution.\n",
      "At this point we have a weird decision to make, how many neurons in the hidden layer?\n",
      "Train on 360 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3998 - acc: 0.2611 - val_loss: 1.3734 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "360/360 [==============================] - 0s 96us/step - loss: 1.3622 - acc: 0.3361 - val_loss: 1.3436 - val_acc: 0.4500\n",
      "Epoch 3/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3409 - acc: 0.4361 - val_loss: 1.3494 - val_acc: 0.4500\n",
      "Epoch 4/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 1.3200 - acc: 0.4583 - val_loss: 1.3223 - val_acc: 0.2500\n",
      "Epoch 5/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 1.2956 - acc: 0.4806 - val_loss: 1.2714 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "360/360 [==============================] - 0s 106us/step - loss: 1.2530 - acc: 0.4750 - val_loss: 1.2650 - val_acc: 0.4250\n",
      "Epoch 7/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.2183 - acc: 0.5306 - val_loss: 1.1799 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.1722 - acc: 0.4750 - val_loss: 1.1270 - val_acc: 0.5750\n",
      "Epoch 9/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.1136 - acc: 0.5111 - val_loss: 1.0860 - val_acc: 0.5250\n",
      "Epoch 10/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.0559 - acc: 0.5278 - val_loss: 1.0195 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.9974 - acc: 0.5222 - val_loss: 0.9612 - val_acc: 0.6000\n",
      "Epoch 12/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.9470 - acc: 0.5000 - val_loss: 0.9104 - val_acc: 0.5000\n",
      "Epoch 13/100\n",
      "360/360 [==============================] - 0s 110us/step - loss: 0.9037 - acc: 0.4806 - val_loss: 0.8793 - val_acc: 0.5000\n",
      "Epoch 14/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.8645 - acc: 0.5500 - val_loss: 0.8389 - val_acc: 0.5750\n",
      "Epoch 15/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.8398 - acc: 0.5056 - val_loss: 0.8360 - val_acc: 0.5500\n",
      "Epoch 16/100\n",
      "360/360 [==============================] - 0s 115us/step - loss: 0.8152 - acc: 0.5167 - val_loss: 0.7955 - val_acc: 0.5250\n",
      "Epoch 17/100\n",
      "360/360 [==============================] - 0s 112us/step - loss: 0.7917 - acc: 0.5111 - val_loss: 0.8049 - val_acc: 0.4250\n",
      "Epoch 18/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.7817 - acc: 0.5361 - val_loss: 0.7735 - val_acc: 0.5000\n",
      "Epoch 19/100\n",
      "360/360 [==============================] - 0s 107us/step - loss: 0.7713 - acc: 0.5194 - val_loss: 0.7610 - val_acc: 0.6750\n",
      "Epoch 20/100\n",
      "360/360 [==============================] - 0s 110us/step - loss: 0.7601 - acc: 0.5528 - val_loss: 0.7560 - val_acc: 0.5000\n",
      "Epoch 21/100\n",
      "360/360 [==============================] - 0s 107us/step - loss: 0.7545 - acc: 0.5056 - val_loss: 0.7429 - val_acc: 0.5000\n",
      "Epoch 22/100\n",
      "360/360 [==============================] - 0s 113us/step - loss: 0.7443 - acc: 0.5278 - val_loss: 0.7502 - val_acc: 0.4250\n",
      "Epoch 23/100\n",
      "360/360 [==============================] - 0s 128us/step - loss: 0.7397 - acc: 0.5361 - val_loss: 0.7523 - val_acc: 0.4250\n",
      "Epoch 24/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 0.7385 - acc: 0.5278 - val_loss: 0.7258 - val_acc: 0.5000\n",
      "Epoch 25/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 0.7298 - acc: 0.5306 - val_loss: 0.7310 - val_acc: 0.6250\n",
      "Epoch 26/100\n",
      "360/360 [==============================] - 0s 130us/step - loss: 0.7296 - acc: 0.5139 - val_loss: 0.7326 - val_acc: 0.4250\n",
      "Epoch 27/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.7253 - acc: 0.5194 - val_loss: 0.7132 - val_acc: 0.6500\n",
      "Epoch 28/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.7224 - acc: 0.5361 - val_loss: 0.7110 - val_acc: 0.5250\n",
      "Epoch 29/100\n",
      "360/360 [==============================] - 0s 113us/step - loss: 0.7214 - acc: 0.5417 - val_loss: 0.7150 - val_acc: 0.5750\n",
      "Epoch 30/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.7179 - acc: 0.5083 - val_loss: 0.7099 - val_acc: 0.6250\n",
      "Epoch 31/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.7157 - acc: 0.5056 - val_loss: 0.7086 - val_acc: 0.5750\n",
      "Epoch 32/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.7118 - acc: 0.5389 - val_loss: 0.7058 - val_acc: 0.6250\n",
      "Epoch 33/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.7085 - acc: 0.5667 - val_loss: 0.7252 - val_acc: 0.4250\n",
      "Epoch 34/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.7100 - acc: 0.5500 - val_loss: 0.7101 - val_acc: 0.4250\n",
      "Epoch 35/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.7058 - acc: 0.5806 - val_loss: 0.7123 - val_acc: 0.5000\n",
      "Epoch 36/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.7011 - acc: 0.5444 - val_loss: 0.7018 - val_acc: 0.5000\n",
      "Epoch 37/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.7043 - acc: 0.5778 - val_loss: 0.7075 - val_acc: 0.6250\n",
      "Epoch 38/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.7062 - acc: 0.5417 - val_loss: 0.6932 - val_acc: 0.6750\n",
      "Epoch 39/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.7026 - acc: 0.5694 - val_loss: 0.7046 - val_acc: 0.6250\n",
      "Epoch 40/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.7027 - acc: 0.5611 - val_loss: 0.6960 - val_acc: 0.6500\n",
      "Epoch 41/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.6988 - acc: 0.5944 - val_loss: 0.7225 - val_acc: 0.4250\n",
      "Epoch 42/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.7002 - acc: 0.5583 - val_loss: 0.6897 - val_acc: 0.6750\n",
      "Epoch 43/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.6946 - acc: 0.5556 - val_loss: 0.6968 - val_acc: 0.5250\n",
      "Epoch 44/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.6963 - acc: 0.5944 - val_loss: 0.6850 - val_acc: 0.6500\n",
      "Epoch 45/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.6991 - acc: 0.5639 - val_loss: 0.6889 - val_acc: 0.6500\n",
      "Epoch 46/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.6951 - acc: 0.5722 - val_loss: 0.6947 - val_acc: 0.5500\n",
      "Epoch 47/100\n",
      "360/360 [==============================] - 0s 106us/step - loss: 0.6943 - acc: 0.5722 - val_loss: 0.6955 - val_acc: 0.5000\n",
      "Epoch 48/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.6911 - acc: 0.6167 - val_loss: 0.7100 - val_acc: 0.4250\n",
      "Epoch 49/100\n",
      "360/360 [==============================] - 0s 107us/step - loss: 0.6926 - acc: 0.5722 - val_loss: 0.6896 - val_acc: 0.6500\n",
      "Epoch 50/100\n",
      "360/360 [==============================] - 0s 111us/step - loss: 0.6914 - acc: 0.5472 - val_loss: 0.6828 - val_acc: 0.6250\n",
      "Epoch 51/100\n",
      "360/360 [==============================] - 0s 106us/step - loss: 0.6872 - acc: 0.5917 - val_loss: 0.7154 - val_acc: 0.5000\n",
      "Epoch 52/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.6907 - acc: 0.5889 - val_loss: 0.7075 - val_acc: 0.4500\n",
      "Epoch 53/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 0.6858 - acc: 0.6083 - val_loss: 0.6906 - val_acc: 0.4250\n",
      "Epoch 54/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 0.6854 - acc: 0.6056 - val_loss: 0.6822 - val_acc: 0.6250\n",
      "Epoch 55/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.6868 - acc: 0.5889 - val_loss: 0.6887 - val_acc: 0.6000\n",
      "Epoch 56/100\n",
      "360/360 [==============================] - 0s 111us/step - loss: 0.6808 - acc: 0.6472 - val_loss: 0.6868 - val_acc: 0.5250\n",
      "Epoch 57/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.6817 - acc: 0.6028 - val_loss: 0.6922 - val_acc: 0.4750\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 0s 97us/step - loss: 0.6852 - acc: 0.6056 - val_loss: 0.6923 - val_acc: 0.4500\n",
      "Epoch 59/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.6810 - acc: 0.6000 - val_loss: 0.6833 - val_acc: 0.6000\n",
      "Epoch 60/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.6776 - acc: 0.6139 - val_loss: 0.6965 - val_acc: 0.5500\n",
      "Epoch 61/100\n",
      "360/360 [==============================] - 0s 109us/step - loss: 0.6724 - acc: 0.6194 - val_loss: 0.6708 - val_acc: 0.6250\n",
      "Epoch 62/100\n",
      "360/360 [==============================] - 0s 111us/step - loss: 0.6771 - acc: 0.6028 - val_loss: 0.6682 - val_acc: 0.6750\n",
      "Epoch 63/100\n",
      "360/360 [==============================] - 0s 114us/step - loss: 0.6756 - acc: 0.5694 - val_loss: 0.6696 - val_acc: 0.6750\n",
      "Epoch 64/100\n",
      "360/360 [==============================] - 0s 107us/step - loss: 0.6767 - acc: 0.6139 - val_loss: 0.6932 - val_acc: 0.5750\n",
      "Epoch 65/100\n",
      "360/360 [==============================] - 0s 110us/step - loss: 0.6755 - acc: 0.6083 - val_loss: 0.6859 - val_acc: 0.6500\n",
      "Epoch 66/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.6737 - acc: 0.6222 - val_loss: 0.6697 - val_acc: 0.6750\n",
      "Epoch 67/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.6717 - acc: 0.6278 - val_loss: 0.6772 - val_acc: 0.6250\n",
      "Epoch 68/100\n",
      "360/360 [==============================] - 0s 96us/step - loss: 0.6697 - acc: 0.6389 - val_loss: 0.6755 - val_acc: 0.6250\n",
      "Epoch 69/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.6659 - acc: 0.6250 - val_loss: 0.6796 - val_acc: 0.5750\n",
      "Epoch 70/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.6661 - acc: 0.6500 - val_loss: 0.6744 - val_acc: 0.6000\n",
      "Epoch 71/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.6662 - acc: 0.6250 - val_loss: 0.6729 - val_acc: 0.5500\n",
      "Epoch 72/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 0.6686 - acc: 0.6167 - val_loss: 0.6693 - val_acc: 0.6000\n",
      "Epoch 73/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.6614 - acc: 0.6194 - val_loss: 0.6689 - val_acc: 0.6750\n",
      "Epoch 74/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.6648 - acc: 0.6250 - val_loss: 0.6715 - val_acc: 0.6000\n",
      "Epoch 75/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.6597 - acc: 0.6056 - val_loss: 0.6790 - val_acc: 0.5750\n",
      "Epoch 76/100\n",
      "360/360 [==============================] - 0s 106us/step - loss: 0.6605 - acc: 0.6639 - val_loss: 0.6610 - val_acc: 0.6750\n",
      "Epoch 77/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 0.6574 - acc: 0.6306 - val_loss: 0.6636 - val_acc: 0.5750\n",
      "Epoch 78/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.6582 - acc: 0.6194 - val_loss: 0.6547 - val_acc: 0.6750\n",
      "Epoch 79/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.6575 - acc: 0.6278 - val_loss: 0.6661 - val_acc: 0.6000\n",
      "Epoch 80/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.6552 - acc: 0.6167 - val_loss: 0.6600 - val_acc: 0.6750\n",
      "Epoch 81/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.6494 - acc: 0.6306 - val_loss: 0.6834 - val_acc: 0.5250\n",
      "Epoch 82/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.6535 - acc: 0.6361 - val_loss: 0.6762 - val_acc: 0.5750\n",
      "Epoch 83/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.6489 - acc: 0.6472 - val_loss: 0.6949 - val_acc: 0.3750\n",
      "Epoch 84/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.6467 - acc: 0.6278 - val_loss: 0.6619 - val_acc: 0.6000\n",
      "Epoch 85/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.6480 - acc: 0.6028 - val_loss: 0.6848 - val_acc: 0.5250\n",
      "Epoch 86/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.6458 - acc: 0.6472 - val_loss: 0.6687 - val_acc: 0.5750\n",
      "Epoch 87/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.6456 - acc: 0.6583 - val_loss: 0.6527 - val_acc: 0.6500\n",
      "Epoch 88/100\n",
      "360/360 [==============================] - 0s 123us/step - loss: 0.6486 - acc: 0.6528 - val_loss: 0.6545 - val_acc: 0.6000\n",
      "Epoch 89/100\n",
      "360/360 [==============================] - 0s 107us/step - loss: 0.6401 - acc: 0.6417 - val_loss: 0.6726 - val_acc: 0.5750\n",
      "Epoch 90/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.6422 - acc: 0.6444 - val_loss: 0.6498 - val_acc: 0.6750\n",
      "Epoch 91/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.6361 - acc: 0.6222 - val_loss: 0.6602 - val_acc: 0.5750\n",
      "Epoch 92/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.6332 - acc: 0.6667 - val_loss: 0.6474 - val_acc: 0.5750\n",
      "Epoch 93/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.6344 - acc: 0.6278 - val_loss: 0.6689 - val_acc: 0.5250\n",
      "Epoch 94/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.6355 - acc: 0.6361 - val_loss: 0.6509 - val_acc: 0.6000\n",
      "Epoch 95/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.6332 - acc: 0.6389 - val_loss: 0.6470 - val_acc: 0.6500\n",
      "Epoch 96/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.6282 - acc: 0.6472 - val_loss: 0.6464 - val_acc: 0.7000\n",
      "Epoch 97/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.6280 - acc: 0.6278 - val_loss: 0.6433 - val_acc: 0.6000\n",
      "Epoch 98/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.6257 - acc: 0.6306 - val_loss: 0.6547 - val_acc: 0.5750\n",
      "Epoch 99/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.6300 - acc: 0.6139 - val_loss: 0.6491 - val_acc: 0.5750\n",
      "Epoch 100/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.6256 - acc: 0.6556 - val_loss: 0.6492 - val_acc: 0.6250\n",
      "[('tp', 18), ('tn', 100), ('fp', 0), ('fn', 82)]\n",
      "400/400 [==============================] - 0s 39us/step\n",
      "Scores: [0.6553277611732483, 0.6325]\n",
      "Ok that was neat, it definitely worked better, it had more data though.\n",
      "But what if we help it out, and we sort the values so that the first and last bins are always the min and max values?\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "########################################################################\n",
    "# Experiment 2: can we classify a sample of data?\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "''')\n",
    "print(\"In this example we're going to input 40 values from a single distribution, and we'll see if we can classify the distribution.\")\n",
    "\n",
    "width=40\n",
    "\n",
    "def make_widedataset(width=width):\n",
    "    # we're going to make rows of 40 features unsorted\n",
    "    wlns      = min_max_scale(lognormal(size=(bsize,width))) #log normal\n",
    "    wpowers   = min_max_scale(power(0.1,size=(bsize,width))) #power law\n",
    "    wnorms    = min_max_scale(normal(size=(bsize,width)))    #normal\n",
    "    wuniforms = min_max_scale(uniform(size=(bsize,width)))    #uniform\n",
    "    \n",
    "    wdata = np.concatenate((wlns,wpowers,wnorms,wuniforms))\n",
    "    \n",
    "    # concatenate our labels\n",
    "    wlabels = np.concatenate((\n",
    "        (np.repeat(LOGNORMAL,bsize)),\n",
    "        (np.repeat(POWER,bsize)),\n",
    "        (np.repeat(NORM,bsize)),\n",
    "        (np.repeat(UNIFORM,bsize))))\n",
    "    \n",
    "    joint_shuffle(wdata,wlabels)\n",
    "    wdata = wdata.astype(np.float32)\n",
    "    wlabels = wlabels.astype(np.int32)\n",
    "    wlabels = wlabels.reshape((len(data),))\n",
    "    return wdata, wlabels\n",
    "\n",
    "# make our train sets\n",
    "wdata, wlabels = make_widedataset()\n",
    "# make our test sets\n",
    "test_wdata, test_wlabels = make_widedataset()\n",
    "\n",
    "# split out our validation set\n",
    "wtrain, wvalid = split_validation(90, wdata, wlabels)\n",
    "print(\"At this point we have a weird decision to make, how many neurons in the hidden layer?\")\n",
    "\n",
    "encwc = OneHotEncoder(handle_unknown='ignore')\n",
    "encwc.fit(wtrain[1].reshape(len(wtrain[1]),1))\n",
    "wtrain_y = encwc.transform(wtrain[1].reshape(len(wtrain[1]),1))\n",
    "wvalid_y = encwc.transform(wvalid[1].reshape(len(wvalid[1]),1))\n",
    "wtest_y  = encwc.transform(test_wlabels.reshape(len(test_wlabels),1))\n",
    "\n",
    "# wcnet = theanets.Classifier([width,width/4,4]) #267\n",
    "wcnet = Sequential()\n",
    "wcnet.add(Dense(width,input_shape=(width,),activation=\"sigmoid\"))\n",
    "wcnet.add(Dense(int(width/4),activation=\"sigmoid\"))\n",
    "wcnet.add(Dense(4,activation=\"softmax\"))\n",
    "wcnet.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.1), metrics=[\"accuracy\"])\n",
    "history = wcnet.fit(wtrain[0], wtrain_y, validation_data=(wvalid[0], wvalid_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "score = wcnet.evaluate(test_wdata, wtest_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tp', 18), ('tn', 100), ('fp', 0), ('fn', 82)]\n",
      "400/400 [==============================] - 0s 37us/step\n",
      "Scores: [0.6553277611732483, 0.6325]\n",
      "Ok that was neat, it definitely worked better, it had more data though.\n",
      "But what if we help it out, and we sort the values so that the first and last bins are always the min and max values?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classify = wcnet.predict_classes(test_wdata)\n",
    "print(theautil.classifications(classify,test_wlabels))\n",
    "score = wcnet.evaluate(test_wdata, wtest_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n",
    "# # You could try some of these alternative setups\n",
    "# \n",
    "# [width,4]) #248\n",
    "# [width,width/2,4]) #271\n",
    "# [width,width,4]) #289\n",
    "# [width,width*2,4]) #292\n",
    "# [width,width/2,width/4,4]) #270\n",
    "# [width,width/2,width/4,width/8,width/16,4]) #232\n",
    "# [width,width*8,4]) #304\n",
    "\n",
    "print(\"Ok that was neat, it definitely worked better, it had more data though.\")\n",
    "\n",
    "print(\"But what if we help it out, and we sort the values so that the first and last bins are always the min and max values?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Experiment 3\n",
    "\n",
    "* Given 40 sorted samples what distribution does it come from?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Experiment 3: can we classify a SORTED sample of data?\n",
      "#\n",
      "#\n",
      "#########################################################################\n",
      "\n",
      "Sorting the data\n",
      "Train on 360 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 1.3924 - acc: 0.2444 - val_loss: 1.3864 - val_acc: 0.2000\n",
      "Epoch 2/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3799 - acc: 0.2778 - val_loss: 1.3768 - val_acc: 0.2250\n",
      "Epoch 3/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 1.3707 - acc: 0.3056 - val_loss: 1.3691 - val_acc: 0.4250\n",
      "Epoch 4/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 1.3583 - acc: 0.3278 - val_loss: 1.3585 - val_acc: 0.2000\n",
      "Epoch 5/100\n",
      "360/360 [==============================] - 0s 109us/step - loss: 1.3430 - acc: 0.3750 - val_loss: 1.3349 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3324 - acc: 0.4667 - val_loss: 1.3217 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 1.3105 - acc: 0.5056 - val_loss: 1.2984 - val_acc: 0.4500\n",
      "Epoch 8/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.2823 - acc: 0.5417 - val_loss: 1.2657 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 1.2526 - acc: 0.5806 - val_loss: 1.2292 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "360/360 [==============================] - 0s 122us/step - loss: 1.2074 - acc: 0.5750 - val_loss: 1.1786 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.1588 - acc: 0.5639 - val_loss: 1.1123 - val_acc: 0.5750\n",
      "Epoch 12/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 1.0999 - acc: 0.5528 - val_loss: 1.0554 - val_acc: 0.5750\n",
      "Epoch 13/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 1.0384 - acc: 0.5528 - val_loss: 1.0016 - val_acc: 0.7250\n",
      "Epoch 14/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.9781 - acc: 0.5556 - val_loss: 0.9456 - val_acc: 0.5000\n",
      "Epoch 15/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.9227 - acc: 0.6222 - val_loss: 0.8873 - val_acc: 0.5500\n",
      "Epoch 16/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.8783 - acc: 0.5861 - val_loss: 0.8418 - val_acc: 0.5750\n",
      "Epoch 17/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.8403 - acc: 0.5917 - val_loss: 0.8261 - val_acc: 0.7250\n",
      "Epoch 18/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.8073 - acc: 0.6278 - val_loss: 0.8059 - val_acc: 0.4250\n",
      "Epoch 19/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.7845 - acc: 0.5694 - val_loss: 0.7752 - val_acc: 0.7000\n",
      "Epoch 20/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.7666 - acc: 0.6444 - val_loss: 0.7612 - val_acc: 0.6250\n",
      "Epoch 21/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 0.7489 - acc: 0.7028 - val_loss: 0.7348 - val_acc: 0.6000\n",
      "Epoch 22/100\n",
      "360/360 [==============================] - 0s 109us/step - loss: 0.7399 - acc: 0.6917 - val_loss: 0.7295 - val_acc: 0.7000\n",
      "Epoch 23/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.7247 - acc: 0.6833 - val_loss: 0.7116 - val_acc: 0.6000\n",
      "Epoch 24/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.7184 - acc: 0.7444 - val_loss: 0.7023 - val_acc: 0.6000\n",
      "Epoch 25/100\n",
      "360/360 [==============================] - 0s 116us/step - loss: 0.7109 - acc: 0.6778 - val_loss: 0.7027 - val_acc: 0.9000\n",
      "Epoch 26/100\n",
      "360/360 [==============================] - 0s 116us/step - loss: 0.7000 - acc: 0.6917 - val_loss: 0.7046 - val_acc: 0.6500\n",
      "Epoch 27/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.6970 - acc: 0.7833 - val_loss: 0.6911 - val_acc: 0.6000\n",
      "Epoch 28/100\n",
      "360/360 [==============================] - 0s 116us/step - loss: 0.6852 - acc: 0.7528 - val_loss: 0.6814 - val_acc: 0.9250\n",
      "Epoch 29/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.6735 - acc: 0.7306 - val_loss: 0.7113 - val_acc: 0.4500\n",
      "Epoch 30/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.6730 - acc: 0.7639 - val_loss: 0.6728 - val_acc: 0.6750\n",
      "Epoch 31/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 0.6590 - acc: 0.8333 - val_loss: 0.6762 - val_acc: 0.6750\n",
      "Epoch 32/100\n",
      "360/360 [==============================] - 0s 115us/step - loss: 0.6534 - acc: 0.7944 - val_loss: 0.6496 - val_acc: 0.8000\n",
      "Epoch 33/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 0.6432 - acc: 0.8333 - val_loss: 0.6354 - val_acc: 0.9000\n",
      "Epoch 34/100\n",
      "360/360 [==============================] - 0s 147us/step - loss: 0.6350 - acc: 0.8694 - val_loss: 0.6320 - val_acc: 0.8750\n",
      "Epoch 35/100\n",
      "360/360 [==============================] - 0s 116us/step - loss: 0.6200 - acc: 0.8944 - val_loss: 0.6212 - val_acc: 0.9250\n",
      "Epoch 36/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.6103 - acc: 0.8417 - val_loss: 0.6106 - val_acc: 0.9750\n",
      "Epoch 37/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.5940 - acc: 0.8944 - val_loss: 0.5950 - val_acc: 0.8250\n",
      "Epoch 38/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.5800 - acc: 0.8694 - val_loss: 0.6117 - val_acc: 0.8250\n",
      "Epoch 39/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.5661 - acc: 0.9194 - val_loss: 0.5784 - val_acc: 0.9750\n",
      "Epoch 40/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.5444 - acc: 0.9056 - val_loss: 0.5434 - val_acc: 0.7750\n",
      "Epoch 41/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.5322 - acc: 0.9028 - val_loss: 0.5381 - val_acc: 0.8750\n",
      "Epoch 42/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.5162 - acc: 0.9500 - val_loss: 0.5134 - val_acc: 0.9500\n",
      "Epoch 43/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.4896 - acc: 0.9500 - val_loss: 0.4957 - val_acc: 0.9750\n",
      "Epoch 44/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.4742 - acc: 0.9472 - val_loss: 0.4925 - val_acc: 0.9000\n",
      "Epoch 45/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 0.4530 - acc: 0.9556 - val_loss: 0.4641 - val_acc: 0.9000\n",
      "Epoch 46/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.4297 - acc: 0.9500 - val_loss: 0.4449 - val_acc: 0.9000\n",
      "Epoch 47/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.4122 - acc: 0.9639 - val_loss: 0.4249 - val_acc: 0.9250\n",
      "Epoch 48/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.3948 - acc: 0.9639 - val_loss: 0.4231 - val_acc: 0.9000\n",
      "Epoch 49/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.3781 - acc: 0.9556 - val_loss: 0.3965 - val_acc: 0.9000\n",
      "Epoch 50/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.3567 - acc: 0.9611 - val_loss: 0.3791 - val_acc: 0.9000\n",
      "Epoch 51/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 0.3383 - acc: 0.9639 - val_loss: 0.3752 - val_acc: 0.9000\n",
      "Epoch 52/100\n",
      "360/360 [==============================] - 0s 110us/step - loss: 0.3223 - acc: 0.9639 - val_loss: 0.3489 - val_acc: 0.9250\n",
      "Epoch 53/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.3083 - acc: 0.9667 - val_loss: 0.3350 - val_acc: 0.9000\n",
      "Epoch 54/100\n",
      "360/360 [==============================] - 0s 110us/step - loss: 0.2923 - acc: 0.9667 - val_loss: 0.3300 - val_acc: 0.9000\n",
      "Epoch 55/100\n",
      "360/360 [==============================] - 0s 118us/step - loss: 0.2786 - acc: 0.9667 - val_loss: 0.3075 - val_acc: 0.9250\n",
      "Epoch 56/100\n",
      "360/360 [==============================] - 0s 113us/step - loss: 0.2672 - acc: 0.9694 - val_loss: 0.2973 - val_acc: 0.9750\n",
      "Epoch 57/100\n",
      "360/360 [==============================] - 0s 111us/step - loss: 0.2558 - acc: 0.9639 - val_loss: 0.2866 - val_acc: 0.9750\n",
      "Epoch 58/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.2426 - acc: 0.9722 - val_loss: 0.3083 - val_acc: 0.9000\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 0s 102us/step - loss: 0.2330 - acc: 0.9583 - val_loss: 0.2711 - val_acc: 0.9000\n",
      "Epoch 60/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.2233 - acc: 0.9667 - val_loss: 0.2603 - val_acc: 0.9000\n",
      "Epoch 61/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.2169 - acc: 0.9639 - val_loss: 0.2610 - val_acc: 0.9000\n",
      "Epoch 62/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.2077 - acc: 0.9639 - val_loss: 0.2476 - val_acc: 0.9000\n",
      "Epoch 63/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.1987 - acc: 0.9722 - val_loss: 0.2403 - val_acc: 0.9000\n",
      "Epoch 64/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.1923 - acc: 0.9667 - val_loss: 0.2295 - val_acc: 0.9750\n",
      "Epoch 65/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.1881 - acc: 0.9694 - val_loss: 0.2376 - val_acc: 0.9000\n",
      "Epoch 66/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.1803 - acc: 0.9750 - val_loss: 0.2304 - val_acc: 0.9000\n",
      "Epoch 67/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.1745 - acc: 0.9778 - val_loss: 0.2161 - val_acc: 0.9500\n",
      "Epoch 68/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.1706 - acc: 0.9694 - val_loss: 0.2090 - val_acc: 0.9750\n",
      "Epoch 69/100\n",
      "360/360 [==============================] - 0s 127us/step - loss: 0.1660 - acc: 0.9722 - val_loss: 0.2235 - val_acc: 0.9250\n",
      "Epoch 70/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.1604 - acc: 0.9722 - val_loss: 0.2010 - val_acc: 0.9500\n",
      "Epoch 71/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.1567 - acc: 0.9722 - val_loss: 0.1967 - val_acc: 0.9500\n",
      "Epoch 72/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.1517 - acc: 0.9639 - val_loss: 0.2131 - val_acc: 0.9250\n",
      "Epoch 73/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.1498 - acc: 0.9722 - val_loss: 0.2088 - val_acc: 0.9250\n",
      "Epoch 74/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.1458 - acc: 0.9694 - val_loss: 0.2000 - val_acc: 0.9000\n",
      "Epoch 75/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 0.1432 - acc: 0.9750 - val_loss: 0.1922 - val_acc: 0.9000\n",
      "Epoch 76/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.1388 - acc: 0.9722 - val_loss: 0.1814 - val_acc: 0.9500\n",
      "Epoch 77/100\n",
      "360/360 [==============================] - 0s 132us/step - loss: 0.1357 - acc: 0.9750 - val_loss: 0.1795 - val_acc: 0.9250\n",
      "Epoch 78/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 0.1351 - acc: 0.9722 - val_loss: 0.1731 - val_acc: 0.9750\n",
      "Epoch 79/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.1322 - acc: 0.9722 - val_loss: 0.1818 - val_acc: 0.9000\n",
      "Epoch 80/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.1276 - acc: 0.9722 - val_loss: 0.1691 - val_acc: 0.9500\n",
      "Epoch 81/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.1257 - acc: 0.9694 - val_loss: 0.1667 - val_acc: 0.9500\n",
      "Epoch 82/100\n",
      "360/360 [==============================] - 0s 121us/step - loss: 0.1233 - acc: 0.9750 - val_loss: 0.1673 - val_acc: 0.9250\n",
      "Epoch 83/100\n",
      "360/360 [==============================] - 0s 119us/step - loss: 0.1211 - acc: 0.9750 - val_loss: 0.1739 - val_acc: 0.9000\n",
      "Epoch 84/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.1187 - acc: 0.9750 - val_loss: 0.1726 - val_acc: 0.9000\n",
      "Epoch 85/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.1170 - acc: 0.9750 - val_loss: 0.1561 - val_acc: 0.9750\n",
      "Epoch 86/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.1169 - acc: 0.9722 - val_loss: 0.1683 - val_acc: 0.9000\n",
      "Epoch 87/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.1130 - acc: 0.9722 - val_loss: 0.1640 - val_acc: 0.9000\n",
      "Epoch 88/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.1124 - acc: 0.9778 - val_loss: 0.1602 - val_acc: 0.9000\n",
      "Epoch 89/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.1090 - acc: 0.9778 - val_loss: 0.1604 - val_acc: 0.9000\n",
      "Epoch 90/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.1076 - acc: 0.9750 - val_loss: 0.1462 - val_acc: 0.9750\n",
      "Epoch 91/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.1077 - acc: 0.9778 - val_loss: 0.1485 - val_acc: 0.9500\n",
      "Epoch 92/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.1036 - acc: 0.9722 - val_loss: 0.1538 - val_acc: 0.9000\n",
      "Epoch 93/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.1031 - acc: 0.9778 - val_loss: 0.1425 - val_acc: 0.9500\n",
      "Epoch 94/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.1009 - acc: 0.9778 - val_loss: 0.1429 - val_acc: 0.9500\n",
      "Epoch 95/100\n",
      "360/360 [==============================] - 0s 95us/step - loss: 0.0999 - acc: 0.9778 - val_loss: 0.1501 - val_acc: 0.9000\n",
      "Epoch 96/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.0989 - acc: 0.9778 - val_loss: 0.1361 - val_acc: 0.9750\n",
      "Epoch 97/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.0967 - acc: 0.9750 - val_loss: 0.1338 - val_acc: 0.9750\n",
      "Epoch 98/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.0963 - acc: 0.9750 - val_loss: 0.1379 - val_acc: 0.9500\n",
      "Epoch 99/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.0935 - acc: 0.9778 - val_loss: 0.1304 - val_acc: 0.9750\n",
      "Epoch 100/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.0936 - acc: 0.9778 - val_loss: 0.1385 - val_acc: 0.9250\n",
      "400/400 [==============================] - 0s 36us/step\n",
      "Scores: [0.08292008757591247, 0.9825]\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "########################################################################\n",
    "# Experiment 3: can we classify a SORTED sample of data?\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "''')\n",
    "\n",
    "\n",
    "print(\"Sorting the data\")\n",
    "wdata.sort(axis=1)\n",
    "test_wdata.sort(axis=1)\n",
    "\n",
    "\n",
    "swcnet = Sequential()\n",
    "swcnet.add(Dense(width,input_shape=(width,),activation=\"sigmoid\"))\n",
    "swcnet.add(Dense(int(width/4),activation=\"sigmoid\"))\n",
    "swcnet.add(Dense(4,activation=\"softmax\"))\n",
    "swcnet.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.1), metrics=[\"accuracy\"])\n",
    "history = swcnet.fit(wtrain[0], wtrain_y, validation_data=(wvalid[0], wvalid_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "score = swcnet.evaluate(test_wdata, wtest_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tp', 95), ('tn', 100), ('fp', 0), ('fn', 5)]\n",
      "400/400 [==============================] - 0s 36us/step\n",
      "Scores: [0.08292008757591247, 0.9825]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classify = swcnet.predict_classes(test_wdata)\n",
    "print(theautil.classifications(classify,test_wlabels))\n",
    "score = swcnet.evaluate(test_wdata, wtest_y)\n",
    "print(\"Scores: %s\" % score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was an improvement. What if we do binning instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4\n",
    "\n",
    "* Given 40 histogrammed samples what distribution does it come from?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "# Experiment 4: can we classify a discretized histogram of sample data?\n",
      "#\n",
      "#\n",
      "#########################################################################\n",
      "\n",
      "Apply the histogram to all the data rows\n",
      "Train on 360 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 1.4092 - acc: 0.2417 - val_loss: 1.4054 - val_acc: 0.2250\n",
      "Epoch 2/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3897 - acc: 0.2667 - val_loss: 1.4148 - val_acc: 0.2000\n",
      "Epoch 3/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.3917 - acc: 0.2611 - val_loss: 1.3750 - val_acc: 0.3000\n",
      "Epoch 4/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3930 - acc: 0.2417 - val_loss: 1.3841 - val_acc: 0.3000\n",
      "Epoch 5/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3935 - acc: 0.2556 - val_loss: 1.3793 - val_acc: 0.3000\n",
      "Epoch 6/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 1.3905 - acc: 0.2444 - val_loss: 1.3919 - val_acc: 0.2250\n",
      "Epoch 7/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.3873 - acc: 0.2389 - val_loss: 1.3802 - val_acc: 0.2250\n",
      "Epoch 8/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 1.3921 - acc: 0.2083 - val_loss: 1.3936 - val_acc: 0.2000\n",
      "Epoch 9/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3901 - acc: 0.2611 - val_loss: 1.3774 - val_acc: 0.3000\n",
      "Epoch 10/100\n",
      "360/360 [==============================] - 0s 107us/step - loss: 1.3902 - acc: 0.2333 - val_loss: 1.3802 - val_acc: 0.2750\n",
      "Epoch 11/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3886 - acc: 0.2778 - val_loss: 1.3822 - val_acc: 0.3000\n",
      "Epoch 12/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3877 - acc: 0.2250 - val_loss: 1.3891 - val_acc: 0.2750\n",
      "Epoch 13/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 1.3886 - acc: 0.2528 - val_loss: 1.3808 - val_acc: 0.2750\n",
      "Epoch 14/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3865 - acc: 0.2583 - val_loss: 1.3773 - val_acc: 0.2750\n",
      "Epoch 15/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3837 - acc: 0.2889 - val_loss: 1.3682 - val_acc: 0.3000\n",
      "Epoch 16/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.3868 - acc: 0.2694 - val_loss: 1.3906 - val_acc: 0.2000\n",
      "Epoch 17/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 1.3852 - acc: 0.2556 - val_loss: 1.3967 - val_acc: 0.2000\n",
      "Epoch 18/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3855 - acc: 0.2667 - val_loss: 1.3773 - val_acc: 0.2750\n",
      "Epoch 19/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3834 - acc: 0.2750 - val_loss: 1.3802 - val_acc: 0.2000\n",
      "Epoch 20/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.3796 - acc: 0.2917 - val_loss: 1.3960 - val_acc: 0.2000\n",
      "Epoch 21/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3791 - acc: 0.3000 - val_loss: 1.3829 - val_acc: 0.2250\n",
      "Epoch 22/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3784 - acc: 0.3056 - val_loss: 1.3634 - val_acc: 0.3000\n",
      "Epoch 23/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3784 - acc: 0.2667 - val_loss: 1.3742 - val_acc: 0.2000\n",
      "Epoch 24/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3775 - acc: 0.2667 - val_loss: 1.3735 - val_acc: 0.2250\n",
      "Epoch 25/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 1.3745 - acc: 0.3306 - val_loss: 1.3688 - val_acc: 0.3750\n",
      "Epoch 26/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3732 - acc: 0.3167 - val_loss: 1.3665 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3710 - acc: 0.3639 - val_loss: 1.3660 - val_acc: 0.3250\n",
      "Epoch 28/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3713 - acc: 0.3083 - val_loss: 1.3839 - val_acc: 0.2000\n",
      "Epoch 29/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3697 - acc: 0.3750 - val_loss: 1.3620 - val_acc: 0.5000\n",
      "Epoch 30/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3713 - acc: 0.3722 - val_loss: 1.3574 - val_acc: 0.3500\n",
      "Epoch 31/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.3668 - acc: 0.3194 - val_loss: 1.3580 - val_acc: 0.4750\n",
      "Epoch 32/100\n",
      "360/360 [==============================] - 0s 109us/step - loss: 1.3630 - acc: 0.3583 - val_loss: 1.3471 - val_acc: 0.3000\n",
      "Epoch 33/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3617 - acc: 0.3694 - val_loss: 1.3511 - val_acc: 0.4000\n",
      "Epoch 34/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3581 - acc: 0.3389 - val_loss: 1.3506 - val_acc: 0.4500\n",
      "Epoch 35/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.3544 - acc: 0.3750 - val_loss: 1.3572 - val_acc: 0.2250\n",
      "Epoch 36/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3524 - acc: 0.4083 - val_loss: 1.3370 - val_acc: 0.6000\n",
      "Epoch 37/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3468 - acc: 0.3667 - val_loss: 1.3338 - val_acc: 0.2750\n",
      "Epoch 38/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 1.3458 - acc: 0.3806 - val_loss: 1.3229 - val_acc: 0.5000\n",
      "Epoch 39/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 1.3391 - acc: 0.5083 - val_loss: 1.3360 - val_acc: 0.4000\n",
      "Epoch 40/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 1.3330 - acc: 0.4444 - val_loss: 1.3266 - val_acc: 0.5500\n",
      "Epoch 41/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 1.3282 - acc: 0.5417 - val_loss: 1.3095 - val_acc: 0.5000\n",
      "Epoch 42/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 1.3188 - acc: 0.4833 - val_loss: 1.3319 - val_acc: 0.4250\n",
      "Epoch 43/100\n",
      "360/360 [==============================] - 0s 114us/step - loss: 1.3147 - acc: 0.4889 - val_loss: 1.2952 - val_acc: 0.3250\n",
      "Epoch 44/100\n",
      "360/360 [==============================] - 0s 110us/step - loss: 1.3047 - acc: 0.4806 - val_loss: 1.2841 - val_acc: 0.6500\n",
      "Epoch 45/100\n",
      "360/360 [==============================] - 0s 112us/step - loss: 1.2955 - acc: 0.4694 - val_loss: 1.2870 - val_acc: 0.6000\n",
      "Epoch 46/100\n",
      "360/360 [==============================] - 0s 117us/step - loss: 1.2817 - acc: 0.4889 - val_loss: 1.2673 - val_acc: 0.4750\n",
      "Epoch 47/100\n",
      "360/360 [==============================] - 0s 117us/step - loss: 1.2705 - acc: 0.5056 - val_loss: 1.2372 - val_acc: 0.5750\n",
      "Epoch 48/100\n",
      "360/360 [==============================] - 0s 107us/step - loss: 1.2580 - acc: 0.5361 - val_loss: 1.2397 - val_acc: 0.2750\n",
      "Epoch 49/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 1.2462 - acc: 0.5417 - val_loss: 1.2081 - val_acc: 0.7750\n",
      "Epoch 50/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 1.2317 - acc: 0.5528 - val_loss: 1.1945 - val_acc: 0.5750\n",
      "Epoch 51/100\n",
      "360/360 [==============================] - 0s 106us/step - loss: 1.2088 - acc: 0.5500 - val_loss: 1.1858 - val_acc: 0.4750\n",
      "Epoch 52/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 1.1924 - acc: 0.5806 - val_loss: 1.1661 - val_acc: 0.6000\n",
      "Epoch 53/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 1.1667 - acc: 0.6139 - val_loss: 1.1255 - val_acc: 0.7500\n",
      "Epoch 54/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 1.1460 - acc: 0.5750 - val_loss: 1.0957 - val_acc: 0.5750\n",
      "Epoch 55/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 1.1270 - acc: 0.5389 - val_loss: 1.0856 - val_acc: 0.6250\n",
      "Epoch 56/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.0974 - acc: 0.6278 - val_loss: 1.0449 - val_acc: 0.5000\n",
      "Epoch 57/100\n",
      "360/360 [==============================] - 0s 106us/step - loss: 1.0730 - acc: 0.6111 - val_loss: 1.0254 - val_acc: 0.5000\n",
      "Epoch 58/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 1.0402 - acc: 0.6000 - val_loss: 1.0097 - val_acc: 0.5000\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 0s 100us/step - loss: 1.0221 - acc: 0.6722 - val_loss: 0.9725 - val_acc: 0.6250\n",
      "Epoch 60/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.9950 - acc: 0.6333 - val_loss: 0.9576 - val_acc: 0.7500\n",
      "Epoch 61/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.9761 - acc: 0.6917 - val_loss: 0.9362 - val_acc: 0.6500\n",
      "Epoch 62/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.9507 - acc: 0.6333 - val_loss: 0.9083 - val_acc: 0.5000\n",
      "Epoch 63/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.9317 - acc: 0.6278 - val_loss: 0.8848 - val_acc: 0.5500\n",
      "Epoch 64/100\n",
      "360/360 [==============================] - 0s 95us/step - loss: 0.9144 - acc: 0.6222 - val_loss: 0.8786 - val_acc: 0.6500\n",
      "Epoch 65/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.8952 - acc: 0.7111 - val_loss: 0.8427 - val_acc: 0.7250\n",
      "Epoch 66/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.8736 - acc: 0.7500 - val_loss: 0.8360 - val_acc: 0.6500\n",
      "Epoch 67/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.8585 - acc: 0.7389 - val_loss: 0.8164 - val_acc: 0.6750\n",
      "Epoch 68/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.8430 - acc: 0.7250 - val_loss: 0.8001 - val_acc: 0.7500\n",
      "Epoch 69/100\n",
      "360/360 [==============================] - 0s 97us/step - loss: 0.8255 - acc: 0.7306 - val_loss: 0.8053 - val_acc: 0.7750\n",
      "Epoch 70/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.8106 - acc: 0.7333 - val_loss: 0.7691 - val_acc: 0.6750\n",
      "Epoch 71/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.7992 - acc: 0.7500 - val_loss: 0.7616 - val_acc: 0.6500\n",
      "Epoch 72/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.7812 - acc: 0.7306 - val_loss: 0.7430 - val_acc: 0.7250\n",
      "Epoch 73/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 0.7661 - acc: 0.7250 - val_loss: 0.7312 - val_acc: 0.7750\n",
      "Epoch 74/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.7510 - acc: 0.7500 - val_loss: 0.7124 - val_acc: 0.7500\n",
      "Epoch 75/100\n",
      "360/360 [==============================] - 0s 103us/step - loss: 0.7390 - acc: 0.7389 - val_loss: 0.7201 - val_acc: 0.6750\n",
      "Epoch 76/100\n",
      "360/360 [==============================] - 0s 108us/step - loss: 0.7248 - acc: 0.7528 - val_loss: 0.6841 - val_acc: 0.7500\n",
      "Epoch 77/100\n",
      "360/360 [==============================] - 0s 110us/step - loss: 0.7103 - acc: 0.7444 - val_loss: 0.6801 - val_acc: 0.7750\n",
      "Epoch 78/100\n",
      "360/360 [==============================] - 0s 113us/step - loss: 0.6942 - acc: 0.7667 - val_loss: 0.6674 - val_acc: 0.7750\n",
      "Epoch 79/100\n",
      "360/360 [==============================] - 0s 104us/step - loss: 0.6840 - acc: 0.7389 - val_loss: 0.6625 - val_acc: 0.7000\n",
      "Epoch 80/100\n",
      "360/360 [==============================] - 0s 105us/step - loss: 0.6673 - acc: 0.7667 - val_loss: 0.6495 - val_acc: 0.6750\n",
      "Epoch 81/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.6539 - acc: 0.7833 - val_loss: 0.6379 - val_acc: 0.7000\n",
      "Epoch 82/100\n",
      "360/360 [==============================] - 0s 115us/step - loss: 0.6424 - acc: 0.7722 - val_loss: 0.6490 - val_acc: 0.7000\n",
      "Epoch 83/100\n",
      "360/360 [==============================] - 0s 102us/step - loss: 0.6332 - acc: 0.7639 - val_loss: 0.6146 - val_acc: 0.7000\n",
      "Epoch 84/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.6175 - acc: 0.7806 - val_loss: 0.5923 - val_acc: 0.7750\n",
      "Epoch 85/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.6043 - acc: 0.7694 - val_loss: 0.5871 - val_acc: 0.7750\n",
      "Epoch 86/100\n",
      "360/360 [==============================] - 0s 101us/step - loss: 0.5937 - acc: 0.7917 - val_loss: 0.5774 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.5816 - acc: 0.7722 - val_loss: 0.5661 - val_acc: 0.8000\n",
      "Epoch 88/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.5706 - acc: 0.8278 - val_loss: 0.5627 - val_acc: 0.7000\n",
      "Epoch 89/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.5634 - acc: 0.7833 - val_loss: 0.5468 - val_acc: 0.7750\n",
      "Epoch 90/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.5543 - acc: 0.7694 - val_loss: 0.5489 - val_acc: 0.7000\n",
      "Epoch 91/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.5436 - acc: 0.8111 - val_loss: 0.5393 - val_acc: 0.7000\n",
      "Epoch 92/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.5353 - acc: 0.7833 - val_loss: 0.5357 - val_acc: 0.7000\n",
      "Epoch 93/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.5270 - acc: 0.8111 - val_loss: 0.5229 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.5184 - acc: 0.8139 - val_loss: 0.5115 - val_acc: 0.8000\n",
      "Epoch 95/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.5092 - acc: 0.7944 - val_loss: 0.5226 - val_acc: 0.7000\n",
      "Epoch 96/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.5022 - acc: 0.8167 - val_loss: 0.5001 - val_acc: 0.8000\n",
      "Epoch 97/100\n",
      "360/360 [==============================] - 0s 100us/step - loss: 0.4957 - acc: 0.7944 - val_loss: 0.5015 - val_acc: 0.7000\n",
      "Epoch 98/100\n",
      "360/360 [==============================] - 0s 99us/step - loss: 0.4910 - acc: 0.7722 - val_loss: 0.4895 - val_acc: 0.9500\n",
      "Epoch 99/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.4806 - acc: 0.8417 - val_loss: 0.4789 - val_acc: 0.7750\n",
      "Epoch 100/100\n",
      "360/360 [==============================] - 0s 98us/step - loss: 0.4771 - acc: 0.8167 - val_loss: 0.4804 - val_acc: 1.0000\n",
      "[('tp', 100), ('tn', 89), ('fp', 11), ('fn', 0)]\n",
      "400/400 [==============================] - 0s 36us/step\n",
      "Scores: [0.5151433873176575, 0.9675]\n",
      "Name:log normal Tests:[1000] Count:Counter({1: 847, 0: 153}) -- Res:[('tp', 0), ('tn', 153), ('fp', 847), ('fn', 0)]\n",
      "Name:power Tests:[1000] Count:Counter({1: 1000}) -- Res:[('tp', 1000), ('tn', 0), ('fp', 0), ('fn', 0)]\n",
      "Name:normal Tests:[1000] Count:Counter({2: 998, 3: 2}) -- Res:[('tp', 0), ('tn', 0), ('fp', 0), ('fn', 0)]\n",
      "Name:uniforms Tests:[1000] Count:Counter({3: 999, 0: 1}) -- Res:[('tp', 0), ('tn', 0), ('fp', 0), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "########################################################################\n",
    "# Experiment 4: can we classify a discretized histogram of sample data?\n",
    "#\n",
    "#\n",
    "#########################################################################\n",
    "'''\n",
    ")\n",
    "# let's try actual binning\n",
    "\n",
    "\n",
    "def bin(row):\n",
    "    return np.histogram(row,bins=len(row),range=(0.0,1.0))[0]/float(len(row))\n",
    "\n",
    "print(\"Apply the histogram to all the data rows\")\n",
    "bdata = np.apply_along_axis(bin,1,wdata).astype(np.float32)\n",
    "blabels = wlabels\n",
    "\n",
    "# ensure we have our test data\n",
    "test_bdata = np.apply_along_axis(bin,1,test_wdata).astype(np.float32)\n",
    "test_blabels = test_wlabels\n",
    "\n",
    "# helper data \n",
    "enum_funcs = [\n",
    "    (LOGNORMAL,\"log normal\",lambda size: lognormal(size=size)),\n",
    "    (POWER,\"power\",lambda size: power(0.1,size=size)),\n",
    "    (NORM,\"normal\",lambda size: normal(size=size)),\n",
    "    (UNIFORM,\"uniforms\",lambda size: uniform(size=size)),\n",
    "]\n",
    "\n",
    "# uses enum_funcs to evaluate PER CLASS how well our classify operates\n",
    "def classify_test(bnet,ntests=1000):\n",
    "    for tup in enum_funcs:\n",
    "        enum, name, func = tup\n",
    "        lns = min_max_scale(func(size=(ntests,width))) #log normal\n",
    "        blns = np.apply_along_axis(bin,1,lns).astype(np.float32)\n",
    "        blns_labels = np.repeat(enum,ntests)\n",
    "        blns_labels.astype(np.int32)\n",
    "        classification = bnet.predict_classes(blns)\n",
    "        classified = theautil.classifications(classification,blns_labels)\n",
    "        print(\"Name:%s Tests:[%s] Count:%s -- Res:%s\" % (name,ntests, collections.Counter(classification),classified ))\n",
    "\n",
    "# train & valid\n",
    "btrain, bvalid = split_validation(90, bdata, blabels)\n",
    "\n",
    "encb = OneHotEncoder(handle_unknown='ignore')\n",
    "encb.fit(btrain[1].reshape(len(btrain[1]),1))\n",
    "btrain_y = encb.transform(btrain[1].reshape(len(btrain[1]),1))\n",
    "bvalid_y = encb.transform(bvalid[1].reshape(len(bvalid[1]),1))\n",
    "btest_y  = encb.transform(test_blabels.reshape(len(test_blabels),1))\n",
    "\n",
    "\n",
    "\n",
    "# similar network structure\n",
    "# bnet = theanets.Classifier([width,width/2,4])\n",
    "\n",
    "bnet = Sequential()\n",
    "bnet.add(Dense(width,input_shape=(width,),activation=\"sigmoid\"))\n",
    "bnet.add(Dense(int(width/4),activation=\"sigmoid\"))\n",
    "bnet.add(Dense(4,activation=\"softmax\"))\n",
    "bnet.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.1), metrics=[\"accuracy\"])\n",
    "history = bnet.fit(btrain[0], btrain_y, validation_data=(bvalid[0], bvalid_y),\n",
    "\t            epochs=100, batch_size=16)\n",
    "score = bnet.evaluate(test_bdata, btest_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tp', 100), ('tn', 89), ('fp', 11), ('fn', 0)]\n",
      "400/400 [==============================] - 0s 46us/step\n",
      "Scores: [0.5151433873176575, 0.9675]\n",
      "Name:log normal Tests:[1000] Count:Counter({1: 999, 0: 1}) -- Res:[('tp', 0), ('tn', 1), ('fp', 999), ('fn', 0)]\n",
      "Name:power Tests:[1000] Count:Counter({1: 1000}) -- Res:[('tp', 1000), ('tn', 0), ('fp', 0), ('fn', 0)]\n",
      "Name:normal Tests:[1000] Count:Counter({2: 993, 3: 7}) -- Res:[('tp', 0), ('tn', 0), ('fp', 0), ('fn', 0)]\n",
      "Name:uniforms Tests:[1000] Count:Counter({3: 998, 0: 1, 2: 1}) -- Res:[('tp', 0), ('tn', 0), ('fp', 0), ('fn', 0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classify = bnet.predict_classes(test_bdata)\n",
    "print(theautil.classifications(classify,test_blabels))\n",
    "score = bnet.evaluate(test_bdata, btest_y)\n",
    "print(\"Scores: %s\" % score)\n",
    "\n",
    "classify_test(bnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation: Inputs\n",
    "\n",
    "* For discrete values consider discrete inputs neurons. E.g. if you have 3 letters are your input you should have 3 * 26 input neurons. \n",
    "* Each neuron is \"one-hot\" -- 1 neuron is set to 1 to indicate that 1 discerete value. \n",
    "* An input of AAA would be: \n",
    "  * 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    "* ZZZ would be \n",
    "  * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation: Inputs\n",
    "\n",
    "* For groups of elements consider representing them as their counts.\n",
    "* E.g. 3 cats, 4 dogs, 1 car as: 3 4 1 on 3 input neurons.\n",
    "* Neural networks work well with distributions as inputs and distributions as outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Representation: Words\n",
    "\n",
    "* Words can be represented as word counts where by your vector is the count of each word per document -- you might have a large vocabulary so watch out!\n",
    "* n-grams are popular too with one-hot encoding\n",
    "* Embeddings (a dense vector representation) are popular too. Autoencoded words!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Representaiton: Images\n",
    "\n",
    "* Each neuron can represent a pixel represented from 0 to 1\n",
    "* You can have images as output too!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Representation: Outputs\n",
    "\n",
    "* Do not ask the neural network to distingush discrete values on 1 neuron. Don't expect 1 neuron to output 0.25 for A and 0.9 for B and 1.0 for C. Use 3 neurons!\n",
    "* Distribution outputs are good\n",
    "* Interpretting the output is fine for regression problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "\n",
    "* [Theanets Documentation](https://theanets.readthedocs.org/en/stable/)\n",
    "* [A Practical Guide to TrainingRestricted Boltzmann Machines](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)\n",
    "* [MLP](http://deeplearning.net/tutorial/mlp.html#mlp)\n",
    "* [Deep Learning Tutorials](http://www.iro.umontreal.ca/~pift6266/H10/notes/deepintro.html)\n",
    "* [Deep Learning Tutorials](http://deeplearning.net/tutorial/)\n",
    "* [Coursera: Hinton's Neural Networks for Machine Learning](https://www.coursera.org/course/neuralnets)\n",
    "* [The Next Generation of Neural Networks](https://www.youtube.com/watch?v=AyzOUbkUf3M)\n",
    "* [Geoffrey Hinton: \"Introduction to Deep Learning & Deep Belief Nets\"](https://www.youtube.com/watch?v=GJdWESd543Y)\n",
    "* Bengio's Deep Learning\n",
    "  [(1)](https://www.youtube.com/watch?v=JuimBuvEWBg)[(2)](https://www.youtube.com/watch?v=Fl-W7_z3w3o)\n",
    "* [Nvidia's Deep Learning tutorials](https://developer.nvidia.com/deep-learning-courses\n",
    ")\n",
    "* [Udacity Deep Learning MOOC](https://www.udacity.com/course/deep-learning--ud730)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
